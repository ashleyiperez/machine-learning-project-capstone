{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f4515ca-f99a-4df2-b7ac-2b94874ab111",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Development: Converting model to use built in AWS algorithm for deployment\n",
    "# reference here: https://github.com/aws/amazon-sagemaker-examples/blob/main/introduction_to_amazon_algorithms/lightgbm_catboost_tabular/Amazon_Tabular_Classification_LightGBM_CatBoost.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4351c2d6-d4ce-4998-82eb-412e1e70d4a0",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "pytest-astropy 0.8.0 requires pytest-cov>=2.0, which is not installed.\n",
      "pytest-astropy 0.8.0 requires pytest-filter-subpackage>=0.1, which is not installed.\n",
      "sparkmagic 0.20.4 requires nest-asyncio==1.5.5, but you have nest-asyncio 1.5.6 which is incompatible.\n",
      "awscli 1.27.103 requires rsa<4.8,>=3.1.2, but you have rsa 4.9 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install sagemaker ipywidgets --upgrade --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b2f68f3-044a-42e2-9300-16761fc97ea6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sagemaker, boto3, json\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "aws_role = get_execution_role()\n",
    "aws_region = boto3.Session().region_name\n",
    "sess = sagemaker.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0846282d-ddce-4978-9b1c-628e7664a7b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker import image_uris, model_uris, script_uris\n",
    "\n",
    "\n",
    "train_model_id, train_model_version, train_scope = \"lightgbm-classification-model\", \"*\", \"training\"\n",
    "\n",
    "training_instance_type = \"ml.m5.xlarge\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6fb22d7-f612-4d22-a648-9afd623c1706",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Retrieve the docker image\n",
    "train_image_uri = image_uris.retrieve(\n",
    "    region=None,\n",
    "    framework=None,\n",
    "    model_id=train_model_id,\n",
    "    model_version=train_model_version,\n",
    "    image_scope=train_scope,\n",
    "    instance_type=training_instance_type,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d5ad8dd-2080-4796-b758-942106933e51",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Retrieve the training script\n",
    "train_source_uri = script_uris.retrieve(\n",
    "    model_id=train_model_id, model_version=train_model_version, script_scope=train_scope\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e9954cd5-ca37-4026-a198-2e532ae860e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Retrieve the pre-trained model tarball to further fine-tune\n",
    "train_model_uri = model_uris.retrieve(\n",
    "    model_id=train_model_id, model_version=train_model_version, model_scope=train_scope\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7ea2c9ab-c9bf-4926-85db-303426e2ebe7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Sample training data is available in this bucket\n",
    "training_data_bucket = sess.default_bucket()\n",
    "training_data_prefix = \"capstone/train/\"\n",
    "\n",
    "training_dataset_s3_path = f\"s3://{training_data_bucket}/{training_data_prefix}\"\n",
    "\n",
    "output_bucket = sess.default_bucket()\n",
    "output_prefix = \"capstone-training\"\n",
    "\n",
    "s3_output_location = f\"s3://{output_bucket}/{output_prefix}/output\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "70ebd8d5-105c-4e61-b3ed-91c2fd0a631a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'num_boost_round': '5000', 'early_stopping_rounds': '30', 'metric': 'auto', 'learning_rate': '0.009', 'num_leaves': '67', 'feature_fraction': '0.74', 'bagging_fraction': '0.53', 'bagging_freq': '5', 'max_depth': '11', 'min_data_in_leaf': '26', 'max_delta_step': '0.0', 'lambda_l1': '0.0', 'lambda_l2': '0.0', 'boosting': 'gbdt', 'min_gain_to_split': '0.0', 'scale_pos_weight': '1.0', 'tree_learner': 'serial', 'feature_fraction_bynode': '1.0', 'is_unbalance': 'False', 'max_bin': '255', 'num_threads': '0', 'verbosity': '1', 'use_dask': 'False'}\n"
     ]
    }
   ],
   "source": [
    "from sagemaker import hyperparameters\n",
    "\n",
    "# Retrieve the default hyper-parameters for fine-tuning the model\n",
    "hyperparameters = hyperparameters.retrieve_default(\n",
    "    model_id=train_model_id, model_version=train_model_version\n",
    ")\n",
    "#missing equivalents for: subsample, min_child_weight, colsample_bytree\n",
    "\n",
    "print(hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8b691752-cbda-4257-98af-97ffd669e538",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.estimator import Estimator\n",
    "from sagemaker.utils import name_from_base\n",
    "\n",
    "training_job_name = name_from_base(f\"capstone-final-{train_model_id}-training\")\n",
    "\n",
    "# Create SageMaker Estimator instance\n",
    "tabular_estimator = Estimator(\n",
    "    role=aws_role,\n",
    "    image_uri=train_image_uri,\n",
    "    source_dir=train_source_uri,\n",
    "    model_uri=train_model_uri,\n",
    "    entry_point=\"transfer_learning.py\",\n",
    "    instance_count=1,\n",
    "    instance_type=training_instance_type,\n",
    "    max_run=200,\n",
    "    hyperparameters=hyperparameters,\n",
    "    output_path=s3_output_location,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "87dec1fe-f691-4ac5-82f0-4917e269c47b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: capstone-final-lightgbm-classification--2023-04-02-21-31-29-192\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-02 21:31:31 Starting - Starting the training job...\n",
      "2023-04-02 21:31:47 Starting - Preparing the instances for training...\n",
      "2023-04-02 21:32:28 Downloading - Downloading input data...\n",
      "2023-04-02 21:32:48 Training - Downloading the training image...\n",
      "2023-04-02 21:33:19 Training - Training image download completed. Training in progress.\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2023-04-02 21:33:25,210 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2023-04-02 21:33:25,211 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2023-04-02 21:33:25,220 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2023-04-02 21:33:25,221 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2023-04-02 21:33:25,589 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.8 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mProcessing ./lib/dask/dask-2022.12.1-py3-none-any.whl\u001b[0m\n",
      "\u001b[34mProcessing ./lib/distributed/distributed-2022.12.1-py3-none-any.whl\u001b[0m\n",
      "\u001b[34mProcessing ./lib/graphviz/graphviz-0.17-py3-none-any.whl\u001b[0m\n",
      "\u001b[34mProcessing ./lib/HeapDict/HeapDict-1.0.1-py3-none-any.whl\u001b[0m\n",
      "\u001b[34mProcessing ./lib/lightgbm/lightgbm-3.3.3-py3-none-manylinux1_x86_64.whl\u001b[0m\n",
      "\u001b[34mProcessing ./lib/locket/locket-1.0.0-py2.py3-none-any.whl\u001b[0m\n",
      "\u001b[34mProcessing ./lib/msgpack/msgpack-1.0.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\u001b[0m\n",
      "\u001b[34mProcessing ./lib/partd/partd-1.3.0-py3-none-any.whl\u001b[0m\n",
      "\u001b[34mProcessing ./lib/sortedcontainers/sortedcontainers-2.4.0-py2.py3-none-any.whl\u001b[0m\n",
      "\u001b[34mProcessing ./lib/tblib/tblib-1.7.0-py2.py3-none-any.whl\u001b[0m\n",
      "\u001b[34mProcessing ./lib/toolz/toolz-0.12.0-py3-none-any.whl\u001b[0m\n",
      "\u001b[34mProcessing ./lib/zict/zict-2.2.0-py2.py3-none-any.whl\u001b[0m\n",
      "\u001b[34mProcessing ./lib/sagemaker_jumpstart_tabular_script_utilities/sagemaker_jumpstart_tabular_script_utilities-1.0.0-py2.py3-none-any.whl\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: cloudpickle>=1.1.1 in /opt/conda/lib/python3.8/site-packages (from dask==2022.12.1->-r requirements.txt (line 1)) (2.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml>=5.3.1 in /opt/conda/lib/python3.8/site-packages (from dask==2022.12.1->-r requirements.txt (line 1)) (5.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: click>=7.0 in /opt/conda/lib/python3.8/site-packages (from dask==2022.12.1->-r requirements.txt (line 1)) (8.0.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec>=0.6.0 in /opt/conda/lib/python3.8/site-packages (from dask==2022.12.1->-r requirements.txt (line 1)) (2021.10.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.8/site-packages (from dask==2022.12.1->-r requirements.txt (line 1)) (21.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: psutil>=5.0 in /opt/conda/lib/python3.8/site-packages (from distributed==2022.12.1->-r requirements.txt (line 2)) (5.6.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3 in /opt/conda/lib/python3.8/site-packages (from distributed==2022.12.1->-r requirements.txt (line 2)) (1.26.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tornado>=6.0.3 in /opt/conda/lib/python3.8/site-packages (from distributed==2022.12.1->-r requirements.txt (line 2)) (6.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jinja2 in /opt/conda/lib/python3.8/site-packages (from distributed==2022.12.1->-r requirements.txt (line 2)) (3.0.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy in /opt/conda/lib/python3.8/site-packages (from lightgbm==3.3.3->-r requirements.txt (line 5)) (1.19.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: scikit-learn!=0.22.0 in /opt/conda/lib/python3.8/site-packages (from lightgbm==3.3.3->-r requirements.txt (line 5)) (0.24.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: scipy in /opt/conda/lib/python3.8/site-packages (from lightgbm==3.3.3->-r requirements.txt (line 5)) (1.7.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: wheel in /opt/conda/lib/python3.8/site-packages (from lightgbm==3.3.3->-r requirements.txt (line 5)) (0.37.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging>=20.0->dask==2022.12.1->-r requirements.txt (line 1)) (2.4.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.8/site-packages (from scikit-learn!=0.22.0->lightgbm==3.3.3->-r requirements.txt (line 5)) (1.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.8/site-packages (from scikit-learn!=0.22.0->lightgbm==3.3.3->-r requirements.txt (line 5)) (2.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.8/site-packages (from jinja2->distributed==2022.12.1->-r requirements.txt (line 2)) (2.0.1)\u001b[0m\n",
      "\u001b[34mInstalling collected packages: toolz, locket, partd, HeapDict, zict, tblib, sortedcontainers, msgpack, dask, sagemaker-jumpstart-tabular-script-utilities, lightgbm, graphviz, distributed\u001b[0m\n",
      "\u001b[34mSuccessfully installed HeapDict-1.0.1 dask-2022.12.1 distributed-2022.12.1 graphviz-0.17 lightgbm-3.3.3 locket-1.0.0 msgpack-1.0.4 partd-1.3.0 sagemaker-jumpstart-tabular-script-utilities-1.0.0 sortedcontainers-2.4.0 tblib-1.7.0 toolz-0.12.0 zict-2.2.0\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m2023-04-02 21:33:28,364 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2023-04-02 21:33:28,375 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2023-04-02 21:33:28,385 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2023-04-02 21:33:28,392 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"model\": \"/opt/ml/input/data/model\",\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"bagging_fraction\": \"0.53\",\n",
      "        \"bagging_freq\": \"5\",\n",
      "        \"boosting\": \"gbdt\",\n",
      "        \"early_stopping_rounds\": \"30\",\n",
      "        \"feature_fraction\": \"0.74\",\n",
      "        \"feature_fraction_bynode\": \"1.0\",\n",
      "        \"is_unbalance\": \"False\",\n",
      "        \"lambda_l1\": \"0.0\",\n",
      "        \"lambda_l2\": \"0.0\",\n",
      "        \"learning_rate\": \"0.009\",\n",
      "        \"max_bin\": \"255\",\n",
      "        \"max_delta_step\": \"0.0\",\n",
      "        \"max_depth\": \"11\",\n",
      "        \"metric\": \"auto\",\n",
      "        \"min_data_in_leaf\": \"26\",\n",
      "        \"min_gain_to_split\": \"0.0\",\n",
      "        \"num_boost_round\": \"5000\",\n",
      "        \"num_leaves\": \"67\",\n",
      "        \"num_threads\": \"0\",\n",
      "        \"scale_pos_weight\": \"1.0\",\n",
      "        \"tree_learner\": \"serial\",\n",
      "        \"use_dask\": \"False\",\n",
      "        \"verbosity\": \"1\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"model\": {\n",
      "            \"ContentType\": \"application/x-sagemaker-model\",\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"capstone-final-lightgbm-classification--2023-04-02-21-31-29-192\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://jumpstart-cache-prod-us-east-1/source-directory-tarballs/lightgbm/transfer_learning/regression/v2.1.1/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"transfer_learning\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.m5.xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.m5.xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"transfer_learning.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"bagging_fraction\":\"0.53\",\"bagging_freq\":\"5\",\"boosting\":\"gbdt\",\"early_stopping_rounds\":\"30\",\"feature_fraction\":\"0.74\",\"feature_fraction_bynode\":\"1.0\",\"is_unbalance\":\"False\",\"lambda_l1\":\"0.0\",\"lambda_l2\":\"0.0\",\"learning_rate\":\"0.009\",\"max_bin\":\"255\",\"max_delta_step\":\"0.0\",\"max_depth\":\"11\",\"metric\":\"auto\",\"min_data_in_leaf\":\"26\",\"min_gain_to_split\":\"0.0\",\"num_boost_round\":\"5000\",\"num_leaves\":\"67\",\"num_threads\":\"0\",\"scale_pos_weight\":\"1.0\",\"tree_learner\":\"serial\",\"use_dask\":\"False\",\"verbosity\":\"1\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=transfer_learning.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.m5.xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.m5.xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"model\":{\"ContentType\":\"application/x-sagemaker-model\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"model\",\"training\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=transfer_learning\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://jumpstart-cache-prod-us-east-1/source-directory-tarballs/lightgbm/transfer_learning/regression/v2.1.1/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"model\":\"/opt/ml/input/data/model\",\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"bagging_fraction\":\"0.53\",\"bagging_freq\":\"5\",\"boosting\":\"gbdt\",\"early_stopping_rounds\":\"30\",\"feature_fraction\":\"0.74\",\"feature_fraction_bynode\":\"1.0\",\"is_unbalance\":\"False\",\"lambda_l1\":\"0.0\",\"lambda_l2\":\"0.0\",\"learning_rate\":\"0.009\",\"max_bin\":\"255\",\"max_delta_step\":\"0.0\",\"max_depth\":\"11\",\"metric\":\"auto\",\"min_data_in_leaf\":\"26\",\"min_gain_to_split\":\"0.0\",\"num_boost_round\":\"5000\",\"num_leaves\":\"67\",\"num_threads\":\"0\",\"scale_pos_weight\":\"1.0\",\"tree_learner\":\"serial\",\"use_dask\":\"False\",\"verbosity\":\"1\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"model\":{\"ContentType\":\"application/x-sagemaker-model\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"capstone-final-lightgbm-classification--2023-04-02-21-31-29-192\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://jumpstart-cache-prod-us-east-1/source-directory-tarballs/lightgbm/transfer_learning/regression/v2.1.1/sourcedir.tar.gz\",\"module_name\":\"transfer_learning\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.m5.xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.m5.xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"transfer_learning.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--bagging_fraction\",\"0.53\",\"--bagging_freq\",\"5\",\"--boosting\",\"gbdt\",\"--early_stopping_rounds\",\"30\",\"--feature_fraction\",\"0.74\",\"--feature_fraction_bynode\",\"1.0\",\"--is_unbalance\",\"False\",\"--lambda_l1\",\"0.0\",\"--lambda_l2\",\"0.0\",\"--learning_rate\",\"0.009\",\"--max_bin\",\"255\",\"--max_delta_step\",\"0.0\",\"--max_depth\",\"11\",\"--metric\",\"auto\",\"--min_data_in_leaf\",\"26\",\"--min_gain_to_split\",\"0.0\",\"--num_boost_round\",\"5000\",\"--num_leaves\",\"67\",\"--num_threads\",\"0\",\"--scale_pos_weight\",\"1.0\",\"--tree_learner\",\"serial\",\"--use_dask\",\"False\",\"--verbosity\",\"1\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_MODEL=/opt/ml/input/data/model\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mSM_HP_BAGGING_FRACTION=0.53\u001b[0m\n",
      "\u001b[34mSM_HP_BAGGING_FREQ=5\u001b[0m\n",
      "\u001b[34mSM_HP_BOOSTING=gbdt\u001b[0m\n",
      "\u001b[34mSM_HP_EARLY_STOPPING_ROUNDS=30\u001b[0m\n",
      "\u001b[34mSM_HP_FEATURE_FRACTION=0.74\u001b[0m\n",
      "\u001b[34mSM_HP_FEATURE_FRACTION_BYNODE=1.0\u001b[0m\n",
      "\u001b[34mSM_HP_IS_UNBALANCE=False\u001b[0m\n",
      "\u001b[34mSM_HP_LAMBDA_L1=0.0\u001b[0m\n",
      "\u001b[34mSM_HP_LAMBDA_L2=0.0\u001b[0m\n",
      "\u001b[34mSM_HP_LEARNING_RATE=0.009\u001b[0m\n",
      "\u001b[34mSM_HP_MAX_BIN=255\u001b[0m\n",
      "\u001b[34mSM_HP_MAX_DELTA_STEP=0.0\u001b[0m\n",
      "\u001b[34mSM_HP_MAX_DEPTH=11\u001b[0m\n",
      "\u001b[34mSM_HP_METRIC=auto\u001b[0m\n",
      "\u001b[34mSM_HP_MIN_DATA_IN_LEAF=26\u001b[0m\n",
      "\u001b[34mSM_HP_MIN_GAIN_TO_SPLIT=0.0\u001b[0m\n",
      "\u001b[34mSM_HP_NUM_BOOST_ROUND=5000\u001b[0m\n",
      "\u001b[34mSM_HP_NUM_LEAVES=67\u001b[0m\n",
      "\u001b[34mSM_HP_NUM_THREADS=0\u001b[0m\n",
      "\u001b[34mSM_HP_SCALE_POS_WEIGHT=1.0\u001b[0m\n",
      "\u001b[34mSM_HP_TREE_LEARNER=serial\u001b[0m\n",
      "\u001b[34mSM_HP_USE_DASK=False\u001b[0m\n",
      "\u001b[34mSM_HP_VERBOSITY=1\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python38.zip:/opt/conda/lib/python3.8:/opt/conda/lib/python3.8/lib-dynload:/opt/conda/lib/python3.8/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.8 transfer_learning.py --bagging_fraction 0.53 --bagging_freq 5 --boosting gbdt --early_stopping_rounds 30 --feature_fraction 0.74 --feature_fraction_bynode 1.0 --is_unbalance False --lambda_l1 0.0 --lambda_l2 0.0 --learning_rate 0.009 --max_bin 255 --max_delta_step 0.0 --max_depth 11 --metric auto --min_data_in_leaf 26 --min_gain_to_split 0.0 --num_boost_round 5000 --num_leaves 67 --num_threads 0 --scale_pos_weight 1.0 --tree_learner serial --use_dask False --verbosity 1\u001b[0m\n",
      "\u001b[34mINFO:root:Loading data\u001b[0m\n",
      "\u001b[34mINFO:root:'ContentType' is not identified in either training or validation data channel. Default ContentType 'text/csv' is used to read the train and validation data.\u001b[0m\n",
      "\u001b[34mdata frame ['/opt/ml/input/data/training/data.csv']???             0   1   2   3   4     5   6   7   ...  12  13  14  15  16    17  18  19\u001b[0m\n",
      "\u001b[34m0          NaN   0   1   2   3     4   5   6  ...  11  12  13  14  15    16  17  18\u001b[0m\n",
      "\u001b[34m1          0.0   1   2  10  12  2017   4   0  ...   3   3   2  10  12  2017   4  14\u001b[0m\n",
      "\u001b[34m2          1.0   1   2   7  12  2014   2  16  ...   3   2   2   7  12  2014   0  10\u001b[0m\n",
      "\u001b[34m3          2.0   1   2   6   3  2014   2  17  ...   3   2   2   6   3  2014   0  14\u001b[0m\n",
      "\u001b[34m4          3.0   1   2  10   4  2014   1  15  ...   3   1   2  10   4  2014   6  15\u001b[0m\n",
      "\u001b[34m...        ...  ..  ..  ..  ..   ...  ..  ..  ...  ..  ..  ..  ..  ..   ...  ..  ..\u001b[0m\n",
      "\u001b[34m79657  79667.0   1   4   0   3  2018   4  18  ...   3   3   4   0   3  2018   4  16\u001b[0m\n",
      "\u001b[34m79658  79668.0   1   4   2   3  2018   4  18  ...   3   4   4   2   3  2018   4  17\u001b[0m\n",
      "\u001b[34m79659  79669.0   1   4   1   3  2018   4  18  ...   3   4   4   1   3  2018   4  17\u001b[0m\n",
      "\u001b[34m79660  79670.0   1   1   0   3  2018   2  12  ...   3   3   1   0   3  2018   4  18\u001b[0m\n",
      "\u001b[34m79661  79671.0   1   3  10   3  2018   0  13  ...   0   3   3  10   3  2018   4  18\u001b[0m\n",
      "\u001b[34m[79662 rows x 20 columns]\u001b[0m\n",
      "\u001b[34mINFO:root:Validation data is not found. 20.0% of training data is randomly selected as validation data. The seed for random sampling is 200.\u001b[0m\n",
      "\u001b[34mINFO:root:Found categorical feature indexes file. The categorical column indexes are: [0, 4, 6, 7, 8, 9, 10, 11, 12, 16].\u001b[0m\n",
      "\u001b[34mINFO:root:'_input_model_extracted/__models_info__.json' file could not be found.\u001b[0m\n",
      "\u001b[34mINFO:root:Beginning training\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/lightgbm/basic.py:2068: UserWarning: categorical_feature in Dataset is overridden.\u001b[0m\n",
      "\u001b[34mNew categorical_feature is [0, 4, 6, 7, 8, 9, 10, 11, 12, 16]\n",
      "  _log_warning('categorical_feature in Dataset is overridden.\\n'\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] Met categorical feature which contains sparse values. Consider renumbering to consecutive integers started from zero\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning]\u001b[0m\n",
      "\u001b[34mMet categorical feature which contains sparse values. Consider renumbering to consecutive integers started from zero\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002929 seconds.\u001b[0m\n",
      "\u001b[34mYou can set `force_row_wise=true` to remove the overhead.\u001b[0m\n",
      "\u001b[34mAnd if memory is not enough, you can set `force_col_wise=true`.\u001b[0m\n",
      "\u001b[34m[LightGBM] [Info] Total Bins 4785\u001b[0m\n",
      "\u001b[34m[LightGBM] [Info] Number of data points in the train set: 63730, number of used features: 19\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/lightgbm/basic.py:1780: UserWarning: Overriding the parameters from Reference Dataset.\n",
      "  _log_warning('Overriding the parameters from Reference Dataset.')\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/lightgbm/basic.py:1513: UserWarning: categorical_column in param dict is overridden.\n",
      "  _log_warning(f'{cat_alias} in param dict is overridden.')\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\u001b[0m\n",
      "\u001b[34m[LightGBM] [Info] Start training from score 39837.753397\u001b[0m\n",
      "\u001b[34m[1]#011train's rmse: 22813.3#011val's rmse: 22866.8\u001b[0m\n",
      "\u001b[34mTraining until validation scores don't improve for 30 rounds\u001b[0m\n",
      "\u001b[34m[2]#011train's rmse: 22644.8#011val's rmse: 22698.9\u001b[0m\n",
      "\u001b[34m[3]#011train's rmse: 22626#011val's rmse: 22681.5\u001b[0m\n",
      "\u001b[34m[4]#011train's rmse: 22452.9#011val's rmse: 22508.9\u001b[0m\n",
      "\u001b[34m[5]#011train's rmse: 22281.9#011val's rmse: 22338.1\u001b[0m\n",
      "\u001b[34m[6]#011train's rmse: 22112.4#011val's rmse: 22169.3\u001b[0m\n",
      "\u001b[34m[7]#011train's rmse: 21946.9#011val's rmse: 22003.9\u001b[0m\n",
      "\u001b[34m[8]#011train's rmse: 21780.9#011val's rmse: 21838.6\u001b[0m\n",
      "\u001b[34m[9]#011train's rmse: 21616.7#011val's rmse: 21675.2\u001b[0m\n",
      "\u001b[34m[10]#011train's rmse: 21456#011val's rmse: 21515.3\u001b[0m\n",
      "\u001b[34m[11]#011train's rmse: 21296.2#011val's rmse: 21355.6\u001b[0m\n",
      "\u001b[34m[12]#011train's rmse: 21137#011val's rmse: 21196.7\u001b[0m\n",
      "\u001b[34m[13]#011train's rmse: 20979.2#011val's rmse: 21039.8\u001b[0m\n",
      "\u001b[34m[14]#011train's rmse: 20823.5#011val's rmse: 20884.6\u001b[0m\n",
      "\u001b[34m[15]#011train's rmse: 20671.2#011val's rmse: 20732.9\u001b[0m\n",
      "\u001b[34m[16]#011train's rmse: 20520#011val's rmse: 20582.3\u001b[0m\n",
      "\u001b[34m[17]#011train's rmse: 20368.8#011val's rmse: 20432\u001b[0m\n",
      "\u001b[34m[18]#011train's rmse: 20220.9#011val's rmse: 20284.9\u001b[0m\n",
      "\u001b[34m[19]#011train's rmse: 20073.1#011val's rmse: 20137.8\u001b[0m\n",
      "\u001b[34m[20]#011train's rmse: 19926.7#011val's rmse: 19992.2\u001b[0m\n",
      "\u001b[34m[21]#011train's rmse: 19782.1#011val's rmse: 19848.1\u001b[0m\n",
      "\u001b[34m[22]#011train's rmse: 19639.8#011val's rmse: 19706.5\u001b[0m\n",
      "\u001b[34m[23]#011train's rmse: 19500.1#011val's rmse: 19567.6\u001b[0m\n",
      "\u001b[34m[24]#011train's rmse: 19364.3#011val's rmse: 19433.2\u001b[0m\n",
      "\u001b[34m[25]#011train's rmse: 19226.9#011val's rmse: 19296.5\u001b[0m\n",
      "\u001b[34m[26]#011train's rmse: 19091.2#011val's rmse: 19161.8\u001b[0m\n",
      "\u001b[34m[27]#011train's rmse: 18955.4#011val's rmse: 19026.3\u001b[0m\n",
      "\u001b[34m[28]#011train's rmse: 18820.8#011val's rmse: 18892.3\u001b[0m\n",
      "\u001b[34m[29]#011train's rmse: 18688.8#011val's rmse: 18761.2\u001b[0m\n",
      "\u001b[34m[30]#011train's rmse: 18557.2#011val's rmse: 18630.5\u001b[0m\n",
      "\u001b[34m[31]#011train's rmse: 18427#011val's rmse: 18500.8\u001b[0m\n",
      "\u001b[34m[32]#011train's rmse: 18302.5#011val's rmse: 18377\u001b[0m\n",
      "\u001b[34m[33]#011train's rmse: 18176.2#011val's rmse: 18251.6\u001b[0m\n",
      "\u001b[34m[34]#011train's rmse: 18050#011val's rmse: 18126.2\u001b[0m\n",
      "\u001b[34m[35]#011train's rmse: 17926.5#011val's rmse: 18003.6\u001b[0m\n",
      "\u001b[34m[36]#011train's rmse: 17804#011val's rmse: 17881.9\u001b[0m\n",
      "\u001b[34m[37]#011train's rmse: 17683#011val's rmse: 17762.3\u001b[0m\n",
      "\u001b[34m[38]#011train's rmse: 17561.8#011val's rmse: 17641.9\u001b[0m\n",
      "\u001b[34m[39]#011train's rmse: 17442#011val's rmse: 17522.9\u001b[0m\n",
      "\u001b[34m[40]#011train's rmse: 17324.4#011val's rmse: 17406.4\u001b[0m\n",
      "\u001b[34m[41]#011train's rmse: 17207.2#011val's rmse: 17290.3\u001b[0m\n",
      "\u001b[34m[42]#011train's rmse: 17092.7#011val's rmse: 17177\u001b[0m\n",
      "\u001b[34m[43]#011train's rmse: 16978.8#011val's rmse: 17064\u001b[0m\n",
      "\u001b[34m[44]#011train's rmse: 16867.1#011val's rmse: 16953.6\u001b[0m\n",
      "\u001b[34m[45]#011train's rmse: 16756.1#011val's rmse: 16843.8\u001b[0m\n",
      "\u001b[34m[46]#011train's rmse: 16647#011val's rmse: 16735.6\u001b[0m\n",
      "\u001b[34m[47]#011train's rmse: 16537.7#011val's rmse: 16627\u001b[0m\n",
      "\u001b[34m[48]#011train's rmse: 16429.8#011val's rmse: 16519.9\u001b[0m\n",
      "\u001b[34m[49]#011train's rmse: 16323#011val's rmse: 16414\u001b[0m\n",
      "\u001b[34m[50]#011train's rmse: 16219.2#011val's rmse: 16311.7\u001b[0m\n",
      "\u001b[34m[51]#011train's rmse: 16114.3#011val's rmse: 16208.1\u001b[0m\n",
      "\u001b[34m[52]#011train's rmse: 16011.4#011val's rmse: 16106.5\u001b[0m\n",
      "\u001b[34m[53]#011train's rmse: 15908.7#011val's rmse: 16005.3\u001b[0m\n",
      "\u001b[34m[54]#011train's rmse: 15896.3#011val's rmse: 15994.7\u001b[0m\n",
      "\u001b[34m[55]#011train's rmse: 15795.3#011val's rmse: 15895\u001b[0m\n",
      "\u001b[34m[56]#011train's rmse: 15696#011val's rmse: 15796.9\u001b[0m\n",
      "\u001b[34m[57]#011train's rmse: 15597.8#011val's rmse: 15699.4\u001b[0m\n",
      "\u001b[34m[58]#011train's rmse: 15500.2#011val's rmse: 15603.1\u001b[0m\n",
      "\u001b[34m[59]#011train's rmse: 15404.9#011val's rmse: 15509\u001b[0m\n",
      "\u001b[34m[60]#011train's rmse: 15391.9#011val's rmse: 15497.2\u001b[0m\n",
      "\u001b[34m[61]#011train's rmse: 15296.4#011val's rmse: 15403\u001b[0m\n",
      "\u001b[34m[62]#011train's rmse: 15202.4#011val's rmse: 15310\u001b[0m\n",
      "\u001b[34m[63]#011train's rmse: 15110.3#011val's rmse: 15218.8\u001b[0m\n",
      "\u001b[34m[64]#011train's rmse: 15018.1#011val's rmse: 15128.1\u001b[0m\n",
      "\u001b[34m[65]#011train's rmse: 14927.1#011val's rmse: 15037.9\u001b[0m\n",
      "\u001b[34m[66]#011train's rmse: 14837.1#011val's rmse: 14948.9\u001b[0m\n",
      "\u001b[34m[67]#011train's rmse: 14748.4#011val's rmse: 14861.6\u001b[0m\n",
      "\u001b[34m[68]#011train's rmse: 14660.4#011val's rmse: 14774.6\u001b[0m\n",
      "\u001b[34m[69]#011train's rmse: 14574.3#011val's rmse: 14690\u001b[0m\n",
      "\u001b[34m[70]#011train's rmse: 14563.2#011val's rmse: 14680.7\u001b[0m\n",
      "\u001b[34m[71]#011train's rmse: 14550.9#011val's rmse: 14670.4\u001b[0m\n",
      "\u001b[34m[72]#011train's rmse: 14465.2#011val's rmse: 14586.7\u001b[0m\n",
      "\u001b[34m[73]#011train's rmse: 14380.5#011val's rmse: 14503.2\u001b[0m\n",
      "\u001b[34m[74]#011train's rmse: 14296.8#011val's rmse: 14421.3\u001b[0m\n",
      "\u001b[34m[75]#011train's rmse: 14213.2#011val's rmse: 14339.2\u001b[0m\n",
      "\u001b[34m[76]#011train's rmse: 14131.1#011val's rmse: 14258.2\u001b[0m\n",
      "\u001b[34m[77]#011train's rmse: 14050#011val's rmse: 14178.7\u001b[0m\n",
      "\u001b[34m[78]#011train's rmse: 13970.1#011val's rmse: 14101.1\u001b[0m\n",
      "\u001b[34m[79]#011train's rmse: 13892.2#011val's rmse: 14024.8\u001b[0m\n",
      "\u001b[34m[80]#011train's rmse: 13814#011val's rmse: 13947.8\u001b[0m\n",
      "\u001b[34m[81]#011train's rmse: 13737.4#011val's rmse: 13872.6\u001b[0m\n",
      "\u001b[34m[82]#011train's rmse: 13727.3#011val's rmse: 13864.2\u001b[0m\n",
      "\u001b[34m[83]#011train's rmse: 13651.6#011val's rmse: 13789.3\u001b[0m\n",
      "\u001b[34m[84]#011train's rmse: 13576.4#011val's rmse: 13715.3\u001b[0m\n",
      "\u001b[34m[85]#011train's rmse: 13501.9#011val's rmse: 13642.2\u001b[0m\n",
      "\u001b[34m[86]#011train's rmse: 13428.2#011val's rmse: 13570.4\u001b[0m\n",
      "\u001b[34m[87]#011train's rmse: 13355.8#011val's rmse: 13499.8\u001b[0m\n",
      "\u001b[34m[88]#011train's rmse: 13283.8#011val's rmse: 13429.3\u001b[0m\n",
      "\u001b[34m[89]#011train's rmse: 13214.5#011val's rmse: 13361.3\u001b[0m\n",
      "\u001b[34m[90]#011train's rmse: 13144.9#011val's rmse: 13293.4\u001b[0m\n",
      "\u001b[34m[91]#011train's rmse: 13135.2#011val's rmse: 13284.6\u001b[0m\n",
      "\u001b[34m[92]#011train's rmse: 13125.4#011val's rmse: 13276\u001b[0m\n",
      "\u001b[34m[93]#011train's rmse: 13056.4#011val's rmse: 13209.1\u001b[0m\n",
      "\u001b[34m[94]#011train's rmse: 12988.3#011val's rmse: 13142.6\u001b[0m\n",
      "\u001b[34m[95]#011train's rmse: 12920.9#011val's rmse: 13076.5\u001b[0m\n",
      "\u001b[34m[96]#011train's rmse: 12854.1#011val's rmse: 13011.3\u001b[0m\n",
      "\u001b[34m[97]#011train's rmse: 12789.3#011val's rmse: 12948.3\u001b[0m\n",
      "\u001b[34m[98]#011train's rmse: 12724.3#011val's rmse: 12884.9\u001b[0m\n",
      "\u001b[34m[99]#011train's rmse: 12661.6#011val's rmse: 12824.3\u001b[0m\n",
      "\u001b[34m[100]#011train's rmse: 12598#011val's rmse: 12762.1\u001b[0m\n",
      "\u001b[34m[101]#011train's rmse: 12535.5#011val's rmse: 12701.2\u001b[0m\n",
      "\u001b[34m[102]#011train's rmse: 12474#011val's rmse: 12642.1\u001b[0m\n",
      "\u001b[34m[103]#011train's rmse: 12414.3#011val's rmse: 12584.4\u001b[0m\n",
      "\u001b[34m[104]#011train's rmse: 12353.6#011val's rmse: 12525.4\u001b[0m\n",
      "\u001b[34m[105]#011train's rmse: 12294#011val's rmse: 12467.3\u001b[0m\n",
      "\u001b[34m[106]#011train's rmse: 12234.8#011val's rmse: 12410.6\u001b[0m\n",
      "\u001b[34m[107]#011train's rmse: 12177.6#011val's rmse: 12355.8\u001b[0m\n",
      "\u001b[34m[108]#011train's rmse: 12120.7#011val's rmse: 12301\u001b[0m\n",
      "\u001b[34m[109]#011train's rmse: 12064#011val's rmse: 12246.1\u001b[0m\n",
      "\u001b[34m[110]#011train's rmse: 12008.2#011val's rmse: 12192.2\u001b[0m\n",
      "\u001b[34m[111]#011train's rmse: 11954.1#011val's rmse: 12139.9\u001b[0m\n",
      "\u001b[34m[112]#011train's rmse: 11899.4#011val's rmse: 12086.7\u001b[0m\n",
      "\u001b[34m[113]#011train's rmse: 11846.6#011val's rmse: 12035.6\u001b[0m\n",
      "\u001b[34m[114]#011train's rmse: 11794.8#011val's rmse: 11985.9\u001b[0m\n",
      "\u001b[34m[115]#011train's rmse: 11787#011val's rmse: 11979.8\u001b[0m\n",
      "\u001b[34m[116]#011train's rmse: 11735.2#011val's rmse: 11929\u001b[0m\n",
      "\u001b[34m[117]#011train's rmse: 11683.5#011val's rmse: 11878.9\u001b[0m\n",
      "\u001b[34m[118]#011train's rmse: 11676.6#011val's rmse: 11874\u001b[0m\n",
      "\u001b[34m[119]#011train's rmse: 11625.5#011val's rmse: 11825.2\u001b[0m\n",
      "\u001b[34m[120]#011train's rmse: 11575.2#011val's rmse: 11776.5\u001b[0m\n",
      "\u001b[34m[121]#011train's rmse: 11525.4#011val's rmse: 11728.5\u001b[0m\n",
      "\u001b[34m[122]#011train's rmse: 11476.3#011val's rmse: 11680.9\u001b[0m\n",
      "\u001b[34m[123]#011train's rmse: 11428.5#011val's rmse: 11635.6\u001b[0m\n",
      "\u001b[34m[124]#011train's rmse: 11381.5#011val's rmse: 11591\u001b[0m\n",
      "\u001b[34m[125]#011train's rmse: 11334.6#011val's rmse: 11546.7\u001b[0m\n",
      "\u001b[34m[126]#011train's rmse: 11288.8#011val's rmse: 11503.1\u001b[0m\n",
      "\u001b[34m[127]#011train's rmse: 11242.6#011val's rmse: 11459.3\u001b[0m\n",
      "\u001b[34m[128]#011train's rmse: 11197.6#011val's rmse: 11416\u001b[0m\n",
      "\u001b[34m[129]#011train's rmse: 11153.2#011val's rmse: 11373\u001b[0m\n",
      "\u001b[34m[130]#011train's rmse: 11109.2#011val's rmse: 11331\u001b[0m\n",
      "\u001b[34m[131]#011train's rmse: 11065.5#011val's rmse: 11288.8\u001b[0m\n",
      "\u001b[34m[132]#011train's rmse: 11022.2#011val's rmse: 11246.6\u001b[0m\n",
      "\u001b[34m[133]#011train's rmse: 10980.6#011val's rmse: 11207.2\u001b[0m\n",
      "\u001b[34m[134]#011train's rmse: 10939.4#011val's rmse: 11167.2\u001b[0m\n",
      "\u001b[34m[135]#011train's rmse: 10897.8#011val's rmse: 11127.6\u001b[0m\n",
      "\u001b[34m[136]#011train's rmse: 10856.4#011val's rmse: 11088.5\u001b[0m\n",
      "\u001b[34m[137]#011train's rmse: 10816.6#011val's rmse: 11050.6\u001b[0m\n",
      "\u001b[34m[138]#011train's rmse: 10776.2#011val's rmse: 11012.6\u001b[0m\n",
      "\u001b[34m[139]#011train's rmse: 10736.4#011val's rmse: 10974.9\u001b[0m\n",
      "\u001b[34m[140]#011train's rmse: 10697#011val's rmse: 10937.3\u001b[0m\n",
      "\u001b[34m[141]#011train's rmse: 10659.2#011val's rmse: 10901.6\u001b[0m\n",
      "\u001b[34m[142]#011train's rmse: 10621.1#011val's rmse: 10865.5\u001b[0m\n",
      "\u001b[34m[143]#011train's rmse: 10583.4#011val's rmse: 10830.1\u001b[0m\n",
      "\u001b[34m[144]#011train's rmse: 10546.9#011val's rmse: 10795.5\u001b[0m\n",
      "\u001b[34m[145]#011train's rmse: 10540.5#011val's rmse: 10790.9\u001b[0m\n",
      "\u001b[34m[146]#011train's rmse: 10505.8#011val's rmse: 10758.1\u001b[0m\n",
      "\u001b[34m[147]#011train's rmse: 10469.7#011val's rmse: 10724.6\u001b[0m\n",
      "\u001b[34m[148]#011train's rmse: 10434.4#011val's rmse: 10691.4\u001b[0m\n",
      "\u001b[34m[149]#011train's rmse: 10399.9#011val's rmse: 10658.7\u001b[0m\n",
      "\u001b[34m[150]#011train's rmse: 10366#011val's rmse: 10626.8\u001b[0m\n",
      "\u001b[34m[151]#011train's rmse: 10332.3#011val's rmse: 10595.6\u001b[0m\n",
      "\u001b[34m[152]#011train's rmse: 10327.1#011val's rmse: 10592.4\u001b[0m\n",
      "\u001b[34m[153]#011train's rmse: 10293.3#011val's rmse: 10561.1\u001b[0m\n",
      "\u001b[34m[154]#011train's rmse: 10261#011val's rmse: 10531.2\u001b[0m\n",
      "\u001b[34m[155]#011train's rmse: 10228#011val's rmse: 10500.6\u001b[0m\n",
      "\u001b[34m[156]#011train's rmse: 10195.7#011val's rmse: 10470.8\u001b[0m\n",
      "\u001b[34m[157]#011train's rmse: 10163.4#011val's rmse: 10441.2\u001b[0m\n",
      "\u001b[34m[158]#011train's rmse: 10131.6#011val's rmse: 10411.3\u001b[0m\n",
      "\u001b[34m[159]#011train's rmse: 10100.8#011val's rmse: 10382.5\u001b[0m\n",
      "\u001b[34m[160]#011train's rmse: 10070#011val's rmse: 10353.8\u001b[0m\n",
      "\u001b[34m[161]#011train's rmse: 10039.9#011val's rmse: 10325.2\u001b[0m\n",
      "\u001b[34m[162]#011train's rmse: 10010.3#011val's rmse: 10297.5\u001b[0m\n",
      "\u001b[34m[163]#011train's rmse: 9981.43#011val's rmse: 10271.2\u001b[0m\n",
      "\u001b[34m[164]#011train's rmse: 9953.17#011val's rmse: 10245.1\u001b[0m\n",
      "\u001b[34m[165]#011train's rmse: 9925.27#011val's rmse: 10219.4\u001b[0m\n",
      "\u001b[34m[166]#011train's rmse: 9896.85#011val's rmse: 10193.5\u001b[0m\n",
      "\u001b[34m[167]#011train's rmse: 9868.77#011val's rmse: 10168.1\u001b[0m\n",
      "\u001b[34m[168]#011train's rmse: 9840.63#011val's rmse: 10142.5\u001b[0m\n",
      "\u001b[34m[169]#011train's rmse: 9812.85#011val's rmse: 10117.3\u001b[0m\n",
      "\u001b[34m[170]#011train's rmse: 9786.6#011val's rmse: 10093.5\u001b[0m\n",
      "\u001b[34m[171]#011train's rmse: 9760.77#011val's rmse: 10069.3\u001b[0m\n",
      "\u001b[34m[172]#011train's rmse: 9734.17#011val's rmse: 10045\u001b[0m\n",
      "\u001b[34m[173]#011train's rmse: 9708.31#011val's rmse: 10021.2\u001b[0m\n",
      "\u001b[34m[174]#011train's rmse: 9683.44#011val's rmse: 9997.96\u001b[0m\n",
      "\u001b[34m[175]#011train's rmse: 9658.36#011val's rmse: 9975.18\u001b[0m\n",
      "\u001b[34m[176]#011train's rmse: 9633.69#011val's rmse: 9952.49\u001b[0m\n",
      "\u001b[34m[177]#011train's rmse: 9609.67#011val's rmse: 9930.46\u001b[0m\n",
      "\u001b[34m[178]#011train's rmse: 9585.33#011val's rmse: 9908.35\u001b[0m\n",
      "\u001b[34m[179]#011train's rmse: 9562.85#011val's rmse: 9887.77\u001b[0m\n",
      "\u001b[34m[180]#011train's rmse: 9540.13#011val's rmse: 9867.35\u001b[0m\n",
      "\u001b[34m[181]#011train's rmse: 9517.26#011val's rmse: 9846.7\u001b[0m\n",
      "\u001b[34m[182]#011train's rmse: 9494.75#011val's rmse: 9826.59\u001b[0m\n",
      "\u001b[34m[183]#011train's rmse: 9472.49#011val's rmse: 9806.15\u001b[0m\n",
      "\u001b[34m[184]#011train's rmse: 9449.97#011val's rmse: 9786.18\u001b[0m\n",
      "\u001b[34m[185]#011train's rmse: 9427.9#011val's rmse: 9766.66\u001b[0m\n",
      "\u001b[34m[186]#011train's rmse: 9406.25#011val's rmse: 9747.08\u001b[0m\n",
      "\u001b[34m[187]#011train's rmse: 9385.43#011val's rmse: 9729.18\u001b[0m\n",
      "\u001b[34m[188]#011train's rmse: 9365.26#011val's rmse: 9711.14\u001b[0m\n",
      "\u001b[34m[189]#011train's rmse: 9344.97#011val's rmse: 9693.33\u001b[0m\n",
      "\u001b[34m[190]#011train's rmse: 9324.36#011val's rmse: 9675.07\u001b[0m\n",
      "\u001b[34m[191]#011train's rmse: 9304.71#011val's rmse: 9657.04\u001b[0m\n",
      "\u001b[34m[192]#011train's rmse: 9285.14#011val's rmse: 9639.19\u001b[0m\n",
      "\u001b[34m[193]#011train's rmse: 9265.17#011val's rmse: 9621.72\u001b[0m\n",
      "\u001b[34m[194]#011train's rmse: 9245.95#011val's rmse: 9604.61\u001b[0m\n",
      "\u001b[34m[195]#011train's rmse: 9226.59#011val's rmse: 9587.97\u001b[0m\n",
      "\u001b[34m[196]#011train's rmse: 9207.58#011val's rmse: 9570.91\u001b[0m\n",
      "\u001b[34m[197]#011train's rmse: 9189.9#011val's rmse: 9555.04\u001b[0m\n",
      "\u001b[34m[198]#011train's rmse: 9170.8#011val's rmse: 9538.04\u001b[0m\n",
      "\u001b[34m[199]#011train's rmse: 9153.17#011val's rmse: 9522.48\u001b[0m\n",
      "\u001b[34m[200]#011train's rmse: 9135.33#011val's rmse: 9506.47\u001b[0m\n",
      "\u001b[34m[201]#011train's rmse: 9117.83#011val's rmse: 9491.63\u001b[0m\n",
      "\u001b[34m[202]#011train's rmse: 9100.57#011val's rmse: 9477.35\u001b[0m\n",
      "\u001b[34m[203]#011train's rmse: 9083.49#011val's rmse: 9463.03\u001b[0m\n",
      "\u001b[34m[204]#011train's rmse: 9067.06#011val's rmse: 9449.22\u001b[0m\n",
      "\u001b[34m[205]#011train's rmse: 9051.21#011val's rmse: 9435.19\u001b[0m\n",
      "\u001b[34m[206]#011train's rmse: 9034.55#011val's rmse: 9420.66\u001b[0m\n",
      "\u001b[34m[207]#011train's rmse: 9018.94#011val's rmse: 9407.55\u001b[0m\n",
      "\u001b[34m[208]#011train's rmse: 9003.3#011val's rmse: 9394.55\u001b[0m\n",
      "\u001b[34m[209]#011train's rmse: 8987.36#011val's rmse: 9381.26\u001b[0m\n",
      "\u001b[34m[210]#011train's rmse: 8971.63#011val's rmse: 9367.66\u001b[0m\n",
      "\u001b[34m[211]#011train's rmse: 8956.14#011val's rmse: 9353.96\u001b[0m\n",
      "\u001b[34m[212]#011train's rmse: 8940.88#011val's rmse: 9341.14\u001b[0m\n",
      "\u001b[34m[213]#011train's rmse: 8925.8#011val's rmse: 9328.52\u001b[0m\n",
      "\u001b[34m[214]#011train's rmse: 8910.76#011val's rmse: 9315.8\u001b[0m\n",
      "\u001b[34m[215]#011train's rmse: 8896.01#011val's rmse: 9303.71\u001b[0m\n",
      "\u001b[34m[216]#011train's rmse: 8881.37#011val's rmse: 9291\u001b[0m\n",
      "\u001b[34m[217]#011train's rmse: 8877.17#011val's rmse: 9288.23\u001b[0m\n",
      "\u001b[34m[218]#011train's rmse: 8863.53#011val's rmse: 9276.69\u001b[0m\n",
      "\u001b[34m[219]#011train's rmse: 8849.21#011val's rmse: 9264.93\u001b[0m\n",
      "\u001b[34m[220]#011train's rmse: 8835.24#011val's rmse: 9253.59\u001b[0m\n",
      "\u001b[34m[221]#011train's rmse: 8821.73#011val's rmse: 9242.18\u001b[0m\n",
      "\u001b[34m[222]#011train's rmse: 8808.71#011val's rmse: 9230.93\u001b[0m\n",
      "\u001b[34m[223]#011train's rmse: 8795.51#011val's rmse: 9219.88\u001b[0m\n",
      "\u001b[34m[224]#011train's rmse: 8782.65#011val's rmse: 9209.54\u001b[0m\n",
      "\u001b[34m[225]#011train's rmse: 8770.72#011val's rmse: 9199.23\u001b[0m\n",
      "\u001b[34m[226]#011train's rmse: 8757.74#011val's rmse: 9188.45\u001b[0m\n",
      "\u001b[34m[227]#011train's rmse: 8745.46#011val's rmse: 9177.59\u001b[0m\n",
      "\u001b[34m[228]#011train's rmse: 8732.9#011val's rmse: 9167.18\u001b[0m\n",
      "\u001b[34m[229]#011train's rmse: 8720.42#011val's rmse: 9156.85\u001b[0m\n",
      "\u001b[34m[230]#011train's rmse: 8708#011val's rmse: 9147.16\u001b[0m\n",
      "\u001b[34m[231]#011train's rmse: 8696.61#011val's rmse: 9137.19\u001b[0m\n",
      "\u001b[34m[232]#011train's rmse: 8685.5#011val's rmse: 9127.95\u001b[0m\n",
      "\u001b[34m[233]#011train's rmse: 8674.81#011val's rmse: 9119.26\u001b[0m\n",
      "\u001b[34m[234]#011train's rmse: 8663.26#011val's rmse: 9109.94\u001b[0m\n",
      "\u001b[34m[235]#011train's rmse: 8652.31#011val's rmse: 9101.02\u001b[0m\n",
      "\u001b[34m[236]#011train's rmse: 8640.8#011val's rmse: 9092.02\u001b[0m\n",
      "\u001b[34m[237]#011train's rmse: 8636.2#011val's rmse: 9088.81\u001b[0m\n",
      "\u001b[34m[238]#011train's rmse: 8625.04#011val's rmse: 9079.95\u001b[0m\n",
      "\u001b[34m[239]#011train's rmse: 8614.93#011val's rmse: 9071.56\u001b[0m\n",
      "\u001b[34m[240]#011train's rmse: 8604.43#011val's rmse: 9062.46\u001b[0m\n",
      "\u001b[34m[241]#011train's rmse: 8593.94#011val's rmse: 9053.9\u001b[0m\n",
      "\u001b[34m[242]#011train's rmse: 8589.4#011val's rmse: 9051.52\u001b[0m\n",
      "\u001b[34m[243]#011train's rmse: 8579.31#011val's rmse: 9043.38\u001b[0m\n",
      "\u001b[34m[244]#011train's rmse: 8569.57#011val's rmse: 9035.71\u001b[0m\n",
      "\u001b[34m[245]#011train's rmse: 8559.51#011val's rmse: 9027.93\u001b[0m\n",
      "\u001b[34m[246]#011train's rmse: 8548.91#011val's rmse: 9019.21\u001b[0m\n",
      "\u001b[34m[247]#011train's rmse: 8538.29#011val's rmse: 9011.74\u001b[0m\n",
      "\u001b[34m[248]#011train's rmse: 8534.94#011val's rmse: 9010.27\u001b[0m\n",
      "\u001b[34m[249]#011train's rmse: 8524.75#011val's rmse: 9002.52\u001b[0m\n",
      "\u001b[34m[250]#011train's rmse: 8514.57#011val's rmse: 8995.17\u001b[0m\n",
      "\u001b[34m[251]#011train's rmse: 8505.4#011val's rmse: 8987.75\u001b[0m\n",
      "\u001b[34m[252]#011train's rmse: 8496.5#011val's rmse: 8979.88\u001b[0m\n",
      "\u001b[34m[253]#011train's rmse: 8488.25#011val's rmse: 8973.15\u001b[0m\n",
      "\u001b[34m[254]#011train's rmse: 8479.61#011val's rmse: 8966.37\u001b[0m\n",
      "\u001b[34m[255]#011train's rmse: 8476.55#011val's rmse: 8964.74\u001b[0m\n",
      "\u001b[34m[256]#011train's rmse: 8467.62#011val's rmse: 8957.34\u001b[0m\n",
      "\u001b[34m[257]#011train's rmse: 8458.48#011val's rmse: 8949.46\u001b[0m\n",
      "\u001b[34m[258]#011train's rmse: 8449.28#011val's rmse: 8942.36\u001b[0m\n",
      "\u001b[34m[259]#011train's rmse: 8440.11#011val's rmse: 8934.69\u001b[0m\n",
      "\u001b[34m[260]#011train's rmse: 8431.05#011val's rmse: 8927.21\u001b[0m\n",
      "\u001b[34m[261]#011train's rmse: 8422.6#011val's rmse: 8920.91\u001b[0m\n",
      "\u001b[34m[262]#011train's rmse: 8413.86#011val's rmse: 8914.58\u001b[0m\n",
      "\u001b[34m[263]#011train's rmse: 8405.9#011val's rmse: 8909.14\u001b[0m\n",
      "\u001b[34m[264]#011train's rmse: 8397.17#011val's rmse: 8903.09\u001b[0m\n",
      "\u001b[34m[265]#011train's rmse: 8388.62#011val's rmse: 8897.04\u001b[0m\n",
      "\u001b[34m[266]#011train's rmse: 8380.22#011val's rmse: 8890.98\u001b[0m\n",
      "\u001b[34m[267]#011train's rmse: 8371.83#011val's rmse: 8885.6\u001b[0m\n",
      "\u001b[34m[268]#011train's rmse: 8364.24#011val's rmse: 8880.15\u001b[0m\n",
      "\u001b[34m[269]#011train's rmse: 8356.22#011val's rmse: 8874.58\u001b[0m\n",
      "\u001b[34m[270]#011train's rmse: 8348.95#011val's rmse: 8869.37\u001b[0m\n",
      "\u001b[34m[271]#011train's rmse: 8341#011val's rmse: 8863.17\u001b[0m\n",
      "\u001b[34m[272]#011train's rmse: 8333.5#011val's rmse: 8858.02\u001b[0m\n",
      "\u001b[34m[273]#011train's rmse: 8326.46#011val's rmse: 8853.48\u001b[0m\n",
      "\u001b[34m[274]#011train's rmse: 8318.47#011val's rmse: 8847.46\u001b[0m\n",
      "\u001b[34m[275]#011train's rmse: 8311.05#011val's rmse: 8841.92\u001b[0m\n",
      "\u001b[34m[276]#011train's rmse: 8304.7#011val's rmse: 8836.99\u001b[0m\n",
      "\u001b[34m[277]#011train's rmse: 8296.96#011val's rmse: 8831.03\u001b[0m\n",
      "\u001b[34m[278]#011train's rmse: 8289.51#011val's rmse: 8825.33\u001b[0m\n",
      "\u001b[34m[279]#011train's rmse: 8282.66#011val's rmse: 8820.93\u001b[0m\n",
      "\u001b[34m[280]#011train's rmse: 8275.93#011val's rmse: 8816.34\u001b[0m\n",
      "\u001b[34m[281]#011train's rmse: 8269.52#011val's rmse: 8812.39\u001b[0m\n",
      "\u001b[34m[282]#011train's rmse: 8266.6#011val's rmse: 8811.21\u001b[0m\n",
      "\u001b[34m[283]#011train's rmse: 8260#011val's rmse: 8807.57\u001b[0m\n",
      "\u001b[34m[284]#011train's rmse: 8253.78#011val's rmse: 8803.8\u001b[0m\n",
      "\u001b[34m[285]#011train's rmse: 8247.53#011val's rmse: 8799.94\u001b[0m\n",
      "\u001b[34m[286]#011train's rmse: 8240.87#011val's rmse: 8795.76\u001b[0m\n",
      "\u001b[34m[287]#011train's rmse: 8234.17#011val's rmse: 8791.61\u001b[0m\n",
      "\u001b[34m[288]#011train's rmse: 8228.62#011val's rmse: 8787\u001b[0m\n",
      "\u001b[34m[289]#011train's rmse: 8222.26#011val's rmse: 8783.06\u001b[0m\n",
      "\u001b[34m[290]#011train's rmse: 8215.94#011val's rmse: 8779.21\u001b[0m\n",
      "\u001b[34m[291]#011train's rmse: 8210.19#011val's rmse: 8774.64\u001b[0m\n",
      "\u001b[34m[292]#011train's rmse: 8204.07#011val's rmse: 8770.86\u001b[0m\n",
      "\u001b[34m[293]#011train's rmse: 8198.3#011val's rmse: 8767.42\u001b[0m\n",
      "\u001b[34m[294]#011train's rmse: 8192.67#011val's rmse: 8763.44\u001b[0m\n",
      "\u001b[34m[295]#011train's rmse: 8187.5#011val's rmse: 8759.12\u001b[0m\n",
      "\u001b[34m[296]#011train's rmse: 8181.37#011val's rmse: 8754.81\u001b[0m\n",
      "\u001b[34m[297]#011train's rmse: 8175.65#011val's rmse: 8751.3\u001b[0m\n",
      "\u001b[34m[298]#011train's rmse: 8169.71#011val's rmse: 8747.7\u001b[0m\n",
      "\u001b[34m[299]#011train's rmse: 8164.08#011val's rmse: 8743.55\u001b[0m\n",
      "\u001b[34m[300]#011train's rmse: 8158.54#011val's rmse: 8739.69\u001b[0m\n",
      "\u001b[34m[301]#011train's rmse: 8153.58#011val's rmse: 8736.24\u001b[0m\n",
      "\u001b[34m[302]#011train's rmse: 8148.39#011val's rmse: 8732.51\u001b[0m\n",
      "\u001b[34m[303]#011train's rmse: 8142.66#011val's rmse: 8729.24\u001b[0m\n",
      "\u001b[34m[304]#011train's rmse: 8137.14#011val's rmse: 8726.46\u001b[0m\n",
      "\u001b[34m[305]#011train's rmse: 8132.16#011val's rmse: 8723.45\u001b[0m\n",
      "\u001b[34m[306]#011train's rmse: 8125.83#011val's rmse: 8719.88\u001b[0m\n",
      "\u001b[34m[307]#011train's rmse: 8119.57#011val's rmse: 8716.15\u001b[0m\n",
      "\u001b[34m[308]#011train's rmse: 8113.44#011val's rmse: 8712.46\u001b[0m\n",
      "\u001b[34m[309]#011train's rmse: 8107.83#011val's rmse: 8709.16\u001b[0m\n",
      "\u001b[34m[310]#011train's rmse: 8102.83#011val's rmse: 8706.43\u001b[0m\n",
      "\u001b[34m[311]#011train's rmse: 8097.84#011val's rmse: 8702.62\u001b[0m\n",
      "\u001b[34m[312]#011train's rmse: 8093.12#011val's rmse: 8699.27\u001b[0m\n",
      "\u001b[34m[313]#011train's rmse: 8088.51#011val's rmse: 8696.68\u001b[0m\n",
      "\u001b[34m[314]#011train's rmse: 8083.53#011val's rmse: 8694.14\u001b[0m\n",
      "\u001b[34m[315]#011train's rmse: 8078.17#011val's rmse: 8691.06\u001b[0m\n",
      "\u001b[34m[316]#011train's rmse: 8074.18#011val's rmse: 8688.23\u001b[0m\n",
      "\u001b[34m[317]#011train's rmse: 8069.1#011val's rmse: 8685.22\u001b[0m\n",
      "\u001b[34m[318]#011train's rmse: 8063.78#011val's rmse: 8682.34\u001b[0m\n",
      "\u001b[34m[319]#011train's rmse: 8059.08#011val's rmse: 8679.5\u001b[0m\n",
      "\u001b[34m[320]#011train's rmse: 8054.41#011val's rmse: 8676.96\u001b[0m\n",
      "\u001b[34m[321]#011train's rmse: 8049.41#011val's rmse: 8674.76\u001b[0m\n",
      "\u001b[34m[322]#011train's rmse: 8045.22#011val's rmse: 8671.92\u001b[0m\n",
      "\u001b[34m[323]#011train's rmse: 8041.96#011val's rmse: 8670.69\u001b[0m\n",
      "\u001b[34m[324]#011train's rmse: 8037.3#011val's rmse: 8667.69\u001b[0m\n",
      "\u001b[34m[325]#011train's rmse: 8033.44#011val's rmse: 8665.96\u001b[0m\n",
      "\u001b[34m[326]#011train's rmse: 8029.27#011val's rmse: 8663.4\u001b[0m\n",
      "\u001b[34m[327]#011train's rmse: 8024.71#011val's rmse: 8661.04\u001b[0m\n",
      "\u001b[34m[328]#011train's rmse: 8020.23#011val's rmse: 8658.65\u001b[0m\n",
      "\u001b[34m[329]#011train's rmse: 8016.8#011val's rmse: 8656.86\u001b[0m\n",
      "\u001b[34m[330]#011train's rmse: 8012.5#011val's rmse: 8654.53\u001b[0m\n",
      "\u001b[34m[331]#011train's rmse: 8008.06#011val's rmse: 8651.79\u001b[0m\n",
      "\u001b[34m[332]#011train's rmse: 8003.45#011val's rmse: 8648.54\u001b[0m\n",
      "\u001b[34m[333]#011train's rmse: 7999.33#011val's rmse: 8646.74\u001b[0m\n",
      "\u001b[34m[334]#011train's rmse: 7994.91#011val's rmse: 8643.94\u001b[0m\n",
      "\u001b[34m[335]#011train's rmse: 7990.41#011val's rmse: 8640.71\u001b[0m\n",
      "\u001b[34m[336]#011train's rmse: 7986.48#011val's rmse: 8638.71\u001b[0m\n",
      "\u001b[34m[337]#011train's rmse: 7982.17#011val's rmse: 8636.75\u001b[0m\n",
      "\u001b[34m[338]#011train's rmse: 7977.89#011val's rmse: 8633.83\u001b[0m\n",
      "\u001b[34m[339]#011train's rmse: 7974.11#011val's rmse: 8632.37\u001b[0m\n",
      "\u001b[34m[340]#011train's rmse: 7970.24#011val's rmse: 8630.2\u001b[0m\n",
      "\u001b[34m[341]#011train's rmse: 7966.58#011val's rmse: 8628.37\u001b[0m\n",
      "\u001b[34m[342]#011train's rmse: 7962.56#011val's rmse: 8626.19\u001b[0m\n",
      "\u001b[34m[343]#011train's rmse: 7958.33#011val's rmse: 8623.63\u001b[0m\n",
      "\u001b[34m[344]#011train's rmse: 7954.77#011val's rmse: 8621.9\u001b[0m\n",
      "\u001b[34m[345]#011train's rmse: 7951.66#011val's rmse: 8619.88\u001b[0m\n",
      "\u001b[34m[346]#011train's rmse: 7948.11#011val's rmse: 8618.43\u001b[0m\n",
      "\u001b[34m[347]#011train's rmse: 7944.71#011val's rmse: 8617.05\u001b[0m\n",
      "\u001b[34m[348]#011train's rmse: 7941.63#011val's rmse: 8615.34\u001b[0m\n",
      "\u001b[34m[349]#011train's rmse: 7938.62#011val's rmse: 8614\u001b[0m\n",
      "\u001b[34m[350]#011train's rmse: 7935#011val's rmse: 8611.69\u001b[0m\n",
      "\u001b[34m[351]#011train's rmse: 7930.8#011val's rmse: 8610.54\u001b[0m\n",
      "\u001b[34m[352]#011train's rmse: 7927.23#011val's rmse: 8608.77\u001b[0m\n",
      "\u001b[34m[353]#011train's rmse: 7924.29#011val's rmse: 8607.58\u001b[0m\n",
      "\u001b[34m[354]#011train's rmse: 7920.6#011val's rmse: 8606.31\u001b[0m\n",
      "\u001b[34m[355]#011train's rmse: 7917.26#011val's rmse: 8604.84\u001b[0m\n",
      "\u001b[34m[356]#011train's rmse: 7913.94#011val's rmse: 8602.83\u001b[0m\n",
      "\u001b[34m[357]#011train's rmse: 7909.91#011val's rmse: 8601.23\u001b[0m\n",
      "\u001b[34m[358]#011train's rmse: 7905.85#011val's rmse: 8599.26\u001b[0m\n",
      "\u001b[34m[359]#011train's rmse: 7902.25#011val's rmse: 8597.06\u001b[0m\n",
      "\u001b[34m[360]#011train's rmse: 7898.95#011val's rmse: 8596.08\u001b[0m\n",
      "\u001b[34m[361]#011train's rmse: 7895.8#011val's rmse: 8594.23\u001b[0m\n",
      "\u001b[34m[362]#011train's rmse: 7892.45#011val's rmse: 8592.46\u001b[0m\n",
      "\u001b[34m[363]#011train's rmse: 7889.22#011val's rmse: 8590.69\u001b[0m\n",
      "\u001b[34m[364]#011train's rmse: 7886.58#011val's rmse: 8589.77\u001b[0m\n",
      "\u001b[34m[365]#011train's rmse: 7883.43#011val's rmse: 8588.18\u001b[0m\n",
      "\u001b[34m[366]#011train's rmse: 7880.13#011val's rmse: 8586.89\u001b[0m\n",
      "\u001b[34m[367]#011train's rmse: 7877.13#011val's rmse: 8585.9\u001b[0m\n",
      "\u001b[34m[368]#011train's rmse: 7874.51#011val's rmse: 8585.23\u001b[0m\n",
      "\u001b[34m[369]#011train's rmse: 7871.25#011val's rmse: 8584\u001b[0m\n",
      "\u001b[34m[370]#011train's rmse: 7868.16#011val's rmse: 8583.02\u001b[0m\n",
      "\u001b[34m[371]#011train's rmse: 7864.75#011val's rmse: 8582.16\u001b[0m\n",
      "\u001b[34m[372]#011train's rmse: 7861.44#011val's rmse: 8580.27\u001b[0m\n",
      "\u001b[34m[373]#011train's rmse: 7857.8#011val's rmse: 8578.87\u001b[0m\n",
      "\u001b[34m[374]#011train's rmse: 7854.62#011val's rmse: 8577.75\u001b[0m\n",
      "\u001b[34m[375]#011train's rmse: 7851.39#011val's rmse: 8576.03\u001b[0m\n",
      "\u001b[34m[376]#011train's rmse: 7847.84#011val's rmse: 8574.73\u001b[0m\n",
      "\u001b[34m[377]#011train's rmse: 7844.88#011val's rmse: 8573.92\u001b[0m\n",
      "\u001b[34m[378]#011train's rmse: 7841.85#011val's rmse: 8572.72\u001b[0m\n",
      "\u001b[34m[379]#011train's rmse: 7838.81#011val's rmse: 8571.08\u001b[0m\n",
      "\u001b[34m[380]#011train's rmse: 7836.14#011val's rmse: 8569.68\u001b[0m\n",
      "\u001b[34m[381]#011train's rmse: 7833.14#011val's rmse: 8567.83\u001b[0m\n",
      "\u001b[34m[382]#011train's rmse: 7829.41#011val's rmse: 8566.65\u001b[0m\n",
      "\u001b[34m[383]#011train's rmse: 7826.65#011val's rmse: 8565.56\u001b[0m\n",
      "\u001b[34m[384]#011train's rmse: 7824.02#011val's rmse: 8564.8\u001b[0m\n",
      "\u001b[34m[385]#011train's rmse: 7820.78#011val's rmse: 8563.35\u001b[0m\n",
      "\u001b[34m[386]#011train's rmse: 7818.09#011val's rmse: 8562.21\u001b[0m\n",
      "\u001b[34m[387]#011train's rmse: 7814.61#011val's rmse: 8560.58\u001b[0m\n",
      "\u001b[34m[388]#011train's rmse: 7811.28#011val's rmse: 8558.98\u001b[0m\n",
      "\u001b[34m[389]#011train's rmse: 7808.42#011val's rmse: 8557.8\u001b[0m\n",
      "\u001b[34m[390]#011train's rmse: 7805.35#011val's rmse: 8556.54\u001b[0m\n",
      "\u001b[34m[391]#011train's rmse: 7802.99#011val's rmse: 8555.81\u001b[0m\n",
      "\u001b[34m[392]#011train's rmse: 7799.56#011val's rmse: 8554.35\u001b[0m\n",
      "\u001b[34m[393]#011train's rmse: 7796.27#011val's rmse: 8553.41\u001b[0m\n",
      "\u001b[34m[394]#011train's rmse: 7793.36#011val's rmse: 8552.36\u001b[0m\n",
      "\u001b[34m[395]#011train's rmse: 7790.51#011val's rmse: 8551.1\u001b[0m\n",
      "\u001b[34m[396]#011train's rmse: 7787.99#011val's rmse: 8550.35\u001b[0m\n",
      "\u001b[34m[397]#011train's rmse: 7784.89#011val's rmse: 8549.63\u001b[0m\n",
      "\u001b[34m[398]#011train's rmse: 7782.12#011val's rmse: 8548.83\u001b[0m\n",
      "\u001b[34m[399]#011train's rmse: 7779.15#011val's rmse: 8548.02\u001b[0m\n",
      "\u001b[34m[400]#011train's rmse: 7776.12#011val's rmse: 8547.1\u001b[0m\n",
      "\u001b[34m[401]#011train's rmse: 7773.46#011val's rmse: 8545.57\u001b[0m\n",
      "\u001b[34m[402]#011train's rmse: 7770.85#011val's rmse: 8544.73\u001b[0m\n",
      "\u001b[34m[403]#011train's rmse: 7767.83#011val's rmse: 8543.46\u001b[0m\n",
      "\u001b[34m[404]#011train's rmse: 7764.92#011val's rmse: 8542.93\u001b[0m\n",
      "\u001b[34m[405]#011train's rmse: 7762.43#011val's rmse: 8541.84\u001b[0m\n",
      "\u001b[34m[406]#011train's rmse: 7759.78#011val's rmse: 8541.11\u001b[0m\n",
      "\u001b[34m[407]#011train's rmse: 7757.1#011val's rmse: 8539.98\u001b[0m\n",
      "\u001b[34m[408]#011train's rmse: 7754.56#011val's rmse: 8539.27\u001b[0m\n",
      "\u001b[34m[409]#011train's rmse: 7751.84#011val's rmse: 8538.28\u001b[0m\n",
      "\u001b[34m[410]#011train's rmse: 7748.94#011val's rmse: 8537.24\u001b[0m\n",
      "\u001b[34m[411]#011train's rmse: 7746.3#011val's rmse: 8535.91\u001b[0m\n",
      "\u001b[34m[412]#011train's rmse: 7743.96#011val's rmse: 8535.36\u001b[0m\n",
      "\u001b[34m[413]#011train's rmse: 7741.78#011val's rmse: 8534.88\u001b[0m\n",
      "\u001b[34m[414]#011train's rmse: 7739.68#011val's rmse: 8534.09\u001b[0m\n",
      "\u001b[34m[415]#011train's rmse: 7736.62#011val's rmse: 8532.66\u001b[0m\n",
      "\u001b[34m[416]#011train's rmse: 7734.29#011val's rmse: 8532.25\u001b[0m\n",
      "\u001b[34m[417]#011train's rmse: 7732.06#011val's rmse: 8532.05\u001b[0m\n",
      "\u001b[34m[418]#011train's rmse: 7729.95#011val's rmse: 8531.22\u001b[0m\n",
      "\u001b[34m[419]#011train's rmse: 7728.1#011val's rmse: 8530.82\u001b[0m\n",
      "\u001b[34m[420]#011train's rmse: 7725.69#011val's rmse: 8529.95\u001b[0m\n",
      "\u001b[34m[421]#011train's rmse: 7723.42#011val's rmse: 8529.84\u001b[0m\n",
      "\u001b[34m[422]#011train's rmse: 7720.19#011val's rmse: 8528.74\u001b[0m\n",
      "\u001b[34m[423]#011train's rmse: 7717.55#011val's rmse: 8527.93\u001b[0m\n",
      "\u001b[34m[424]#011train's rmse: 7714.64#011val's rmse: 8526.91\u001b[0m\n",
      "\u001b[34m[425]#011train's rmse: 7712.29#011val's rmse: 8526.23\u001b[0m\n",
      "\u001b[34m[426]#011train's rmse: 7709.92#011val's rmse: 8526.02\u001b[0m\n",
      "\u001b[34m[427]#011train's rmse: 7707.35#011val's rmse: 8525.85\u001b[0m\n",
      "\u001b[34m[428]#011train's rmse: 7705.18#011val's rmse: 8524.92\u001b[0m\n",
      "\u001b[34m[429]#011train's rmse: 7702.81#011val's rmse: 8524.42\u001b[0m\n",
      "\u001b[34m[430]#011train's rmse: 7700.51#011val's rmse: 8524.1\u001b[0m\n",
      "\u001b[34m[431]#011train's rmse: 7698.79#011val's rmse: 8523.9\u001b[0m\n",
      "\u001b[34m[432]#011train's rmse: 7696.59#011val's rmse: 8523.82\u001b[0m\n",
      "\u001b[34m[433]#011train's rmse: 7693.9#011val's rmse: 8522.95\u001b[0m\n",
      "\u001b[34m[434]#011train's rmse: 7691.64#011val's rmse: 8522.4\u001b[0m\n",
      "\u001b[34m[435]#011train's rmse: 7689.33#011val's rmse: 8521.62\u001b[0m\n",
      "\u001b[34m[436]#011train's rmse: 7687.04#011val's rmse: 8520.94\u001b[0m\n",
      "\u001b[34m[437]#011train's rmse: 7684.69#011val's rmse: 8519.92\u001b[0m\n",
      "\u001b[34m[438]#011train's rmse: 7682.6#011val's rmse: 8519.31\u001b[0m\n",
      "\u001b[34m[439]#011train's rmse: 7680.61#011val's rmse: 8518.87\u001b[0m\n",
      "\u001b[34m[440]#011train's rmse: 7678.07#011val's rmse: 8518.29\u001b[0m\n",
      "\u001b[34m[441]#011train's rmse: 7676.14#011val's rmse: 8518.26\u001b[0m\n",
      "\u001b[34m[442]#011train's rmse: 7673.66#011val's rmse: 8518.36\u001b[0m\n",
      "\u001b[34m[443]#011train's rmse: 7671.58#011val's rmse: 8518.59\u001b[0m\n",
      "\u001b[34m[444]#011train's rmse: 7669.4#011val's rmse: 8518.92\u001b[0m\n",
      "\u001b[34m[445]#011train's rmse: 7667.12#011val's rmse: 8519.08\u001b[0m\n",
      "\u001b[34m[446]#011train's rmse: 7665.26#011val's rmse: 8518.23\u001b[0m\n",
      "\u001b[34m[447]#011train's rmse: 7662.59#011val's rmse: 8518.02\u001b[0m\n",
      "\u001b[34m[448]#011train's rmse: 7660#011val's rmse: 8518.43\u001b[0m\n",
      "\u001b[34m[449]#011train's rmse: 7658.35#011val's rmse: 8517.86\u001b[0m\n",
      "\u001b[34m[450]#011train's rmse: 7655.94#011val's rmse: 8517.23\u001b[0m\n",
      "\u001b[34m[451]#011train's rmse: 7653.81#011val's rmse: 8516.42\u001b[0m\n",
      "\u001b[34m[452]#011train's rmse: 7651.33#011val's rmse: 8515.8\u001b[0m\n",
      "\u001b[34m[453]#011train's rmse: 7649.46#011val's rmse: 8515.91\u001b[0m\n",
      "\u001b[34m[454]#011train's rmse: 7647.12#011val's rmse: 8515.38\u001b[0m\n",
      "\u001b[34m[455]#011train's rmse: 7644.71#011val's rmse: 8515.11\u001b[0m\n",
      "\u001b[34m[456]#011train's rmse: 7642.92#011val's rmse: 8514.64\u001b[0m\n",
      "\u001b[34m[457]#011train's rmse: 7640.55#011val's rmse: 8515.18\u001b[0m\n",
      "\u001b[34m[458]#011train's rmse: 7637.7#011val's rmse: 8514.63\u001b[0m\n",
      "\u001b[34m[459]#011train's rmse: 7635.51#011val's rmse: 8514.5\u001b[0m\n",
      "\u001b[34m[460]#011train's rmse: 7633.1#011val's rmse: 8513.68\u001b[0m\n",
      "\u001b[34m[461]#011train's rmse: 7631.06#011val's rmse: 8513.09\u001b[0m\n",
      "\u001b[34m[462]#011train's rmse: 7629#011val's rmse: 8512.55\u001b[0m\n",
      "\u001b[34m[463]#011train's rmse: 7627.19#011val's rmse: 8511.65\u001b[0m\n",
      "\u001b[34m[464]#011train's rmse: 7625.29#011val's rmse: 8510.89\u001b[0m\n",
      "\u001b[34m[465]#011train's rmse: 7623.35#011val's rmse: 8510.3\u001b[0m\n",
      "\u001b[34m[466]#011train's rmse: 7621.1#011val's rmse: 8510.24\u001b[0m\n",
      "\u001b[34m[467]#011train's rmse: 7618.71#011val's rmse: 8510.03\u001b[0m\n",
      "\u001b[34m[468]#011train's rmse: 7616.33#011val's rmse: 8509.35\u001b[0m\n",
      "\u001b[34m[469]#011train's rmse: 7614#011val's rmse: 8509.29\u001b[0m\n",
      "\u001b[34m[470]#011train's rmse: 7611.79#011val's rmse: 8509.35\u001b[0m\n",
      "\u001b[34m[471]#011train's rmse: 7609.62#011val's rmse: 8508.77\u001b[0m\n",
      "\u001b[34m[472]#011train's rmse: 7607.53#011val's rmse: 8507.62\u001b[0m\n",
      "\u001b[34m[473]#011train's rmse: 7605.27#011val's rmse: 8506.77\u001b[0m\n",
      "\u001b[34m[474]#011train's rmse: 7603.31#011val's rmse: 8506.06\u001b[0m\n",
      "\u001b[34m[475]#011train's rmse: 7601.41#011val's rmse: 8505.23\u001b[0m\n",
      "\u001b[34m[476]#011train's rmse: 7599.5#011val's rmse: 8504.29\u001b[0m\n",
      "\u001b[34m[477]#011train's rmse: 7597.35#011val's rmse: 8503.61\u001b[0m\n",
      "\u001b[34m[478]#011train's rmse: 7594.71#011val's rmse: 8502.92\u001b[0m\n",
      "\u001b[34m[479]#011train's rmse: 7592.83#011val's rmse: 8502.53\u001b[0m\n",
      "\u001b[34m[480]#011train's rmse: 7590.69#011val's rmse: 8502.26\u001b[0m\n",
      "\u001b[34m[481]#011train's rmse: 7588.5#011val's rmse: 8501.97\u001b[0m\n",
      "\u001b[34m[482]#011train's rmse: 7586.35#011val's rmse: 8500.81\u001b[0m\n",
      "\u001b[34m[483]#011train's rmse: 7584.37#011val's rmse: 8500.35\u001b[0m\n",
      "\u001b[34m[484]#011train's rmse: 7582.36#011val's rmse: 8499.36\u001b[0m\n",
      "\u001b[34m[485]#011train's rmse: 7580.24#011val's rmse: 8499.17\u001b[0m\n",
      "\u001b[34m[486]#011train's rmse: 7578.75#011val's rmse: 8498.86\u001b[0m\n",
      "\u001b[34m[487]#011train's rmse: 7576.62#011val's rmse: 8498.39\u001b[0m\n",
      "\u001b[34m[488]#011train's rmse: 7574.8#011val's rmse: 8498.14\u001b[0m\n",
      "\u001b[34m[489]#011train's rmse: 7573.11#011val's rmse: 8498.32\u001b[0m\n",
      "\u001b[34m[490]#011train's rmse: 7571.27#011val's rmse: 8497.9\u001b[0m\n",
      "\u001b[34m[491]#011train's rmse: 7568.97#011val's rmse: 8497.58\u001b[0m\n",
      "\u001b[34m[492]#011train's rmse: 7566.83#011val's rmse: 8497.07\u001b[0m\n",
      "\u001b[34m[493]#011train's rmse: 7564.62#011val's rmse: 8497.07\u001b[0m\n",
      "\u001b[34m[494]#011train's rmse: 7563.13#011val's rmse: 8496.67\u001b[0m\n",
      "\u001b[34m[495]#011train's rmse: 7561.15#011val's rmse: 8495.98\u001b[0m\n",
      "\u001b[34m[496]#011train's rmse: 7558.58#011val's rmse: 8495.14\u001b[0m\n",
      "\u001b[34m[497]#011train's rmse: 7556.83#011val's rmse: 8494.91\u001b[0m\n",
      "\u001b[34m[498]#011train's rmse: 7554.61#011val's rmse: 8494.92\u001b[0m\n",
      "\u001b[34m[499]#011train's rmse: 7552.24#011val's rmse: 8494.25\u001b[0m\n",
      "\u001b[34m[500]#011train's rmse: 7550.49#011val's rmse: 8493.85\u001b[0m\n",
      "\u001b[34m[501]#011train's rmse: 7548.62#011val's rmse: 8493.7\u001b[0m\n",
      "\u001b[34m[502]#011train's rmse: 7546.64#011val's rmse: 8493.87\u001b[0m\n",
      "\u001b[34m[503]#011train's rmse: 7544.55#011val's rmse: 8493.82\u001b[0m\n",
      "\u001b[34m[504]#011train's rmse: 7542.6#011val's rmse: 8493.9\u001b[0m\n",
      "\u001b[34m[505]#011train's rmse: 7540.48#011val's rmse: 8493.43\u001b[0m\n",
      "\u001b[34m[506]#011train's rmse: 7538.66#011val's rmse: 8492.55\u001b[0m\n",
      "\u001b[34m[507]#011train's rmse: 7536.78#011val's rmse: 8491.53\u001b[0m\n",
      "\u001b[34m[508]#011train's rmse: 7534.73#011val's rmse: 8490.87\u001b[0m\n",
      "\u001b[34m[509]#011train's rmse: 7533.64#011val's rmse: 8490.57\u001b[0m\n",
      "\u001b[34m[510]#011train's rmse: 7531.67#011val's rmse: 8489.78\u001b[0m\n",
      "\u001b[34m[511]#011train's rmse: 7529.84#011val's rmse: 8489.32\u001b[0m\n",
      "\u001b[34m[512]#011train's rmse: 7528.01#011val's rmse: 8489.59\u001b[0m\n",
      "\u001b[34m[513]#011train's rmse: 7525.72#011val's rmse: 8489.54\u001b[0m\n",
      "\u001b[34m[514]#011train's rmse: 7524.28#011val's rmse: 8489.67\u001b[0m\n",
      "\u001b[34m[515]#011train's rmse: 7522.81#011val's rmse: 8489.85\u001b[0m\n",
      "\u001b[34m[516]#011train's rmse: 7520.96#011val's rmse: 8489.78\u001b[0m\n",
      "\u001b[34m[517]#011train's rmse: 7519.05#011val's rmse: 8489.38\u001b[0m\n",
      "\u001b[34m[518]#011train's rmse: 7517.15#011val's rmse: 8489.48\u001b[0m\n",
      "\u001b[34m[519]#011train's rmse: 7515.65#011val's rmse: 8489.16\u001b[0m\n",
      "\u001b[34m[520]#011train's rmse: 7513.87#011val's rmse: 8489.13\u001b[0m\n",
      "\u001b[34m[521]#011train's rmse: 7512.02#011val's rmse: 8488.51\u001b[0m\n",
      "\u001b[34m[522]#011train's rmse: 7510.29#011val's rmse: 8488.26\u001b[0m\n",
      "\u001b[34m[523]#011train's rmse: 7508.6#011val's rmse: 8487.94\u001b[0m\n",
      "\u001b[34m[524]#011train's rmse: 7507.03#011val's rmse: 8487.66\u001b[0m\n",
      "\u001b[34m[525]#011train's rmse: 7505.34#011val's rmse: 8487.48\u001b[0m\n",
      "\u001b[34m[526]#011train's rmse: 7503.23#011val's rmse: 8486.94\u001b[0m\n",
      "\u001b[34m[527]#011train's rmse: 7501.29#011val's rmse: 8486.15\u001b[0m\n",
      "\u001b[34m[528]#011train's rmse: 7499.33#011val's rmse: 8485.98\u001b[0m\n",
      "\u001b[34m[529]#011train's rmse: 7497.77#011val's rmse: 8485.2\u001b[0m\n",
      "\u001b[34m[530]#011train's rmse: 7495.57#011val's rmse: 8484.52\u001b[0m\n",
      "\u001b[34m[531]#011train's rmse: 7494.03#011val's rmse: 8484.29\u001b[0m\n",
      "\u001b[34m[532]#011train's rmse: 7492.51#011val's rmse: 8484.1\u001b[0m\n",
      "\u001b[34m[533]#011train's rmse: 7491.23#011val's rmse: 8483.98\u001b[0m\n",
      "\u001b[34m[534]#011train's rmse: 7489.8#011val's rmse: 8484.03\u001b[0m\n",
      "\u001b[34m[535]#011train's rmse: 7488.19#011val's rmse: 8484.04\u001b[0m\n",
      "\u001b[34m[536]#011train's rmse: 7486.44#011val's rmse: 8482.98\u001b[0m\n",
      "\u001b[34m[537]#011train's rmse: 7484.74#011val's rmse: 8482.63\u001b[0m\n",
      "\u001b[34m[538]#011train's rmse: 7482.92#011val's rmse: 8481.86\u001b[0m\n",
      "\u001b[34m[539]#011train's rmse: 7481.13#011val's rmse: 8481.08\u001b[0m\n",
      "\u001b[34m[540]#011train's rmse: 7479.61#011val's rmse: 8481.1\u001b[0m\n",
      "\u001b[34m[541]#011train's rmse: 7478.17#011val's rmse: 8480.91\u001b[0m\n",
      "\u001b[34m[542]#011train's rmse: 7476.61#011val's rmse: 8480.8\u001b[0m\n",
      "\u001b[34m[543]#011train's rmse: 7474.88#011val's rmse: 8480.44\u001b[0m\n",
      "\u001b[34m[544]#011train's rmse: 7472.9#011val's rmse: 8479.92\u001b[0m\n",
      "\u001b[34m[545]#011train's rmse: 7471.27#011val's rmse: 8479.6\u001b[0m\n",
      "\u001b[34m[546]#011train's rmse: 7469.57#011val's rmse: 8479.77\u001b[0m\n",
      "\u001b[34m[547]#011train's rmse: 7467.81#011val's rmse: 8479.32\u001b[0m\n",
      "\u001b[34m[548]#011train's rmse: 7466.06#011val's rmse: 8478.69\u001b[0m\n",
      "\u001b[34m[549]#011train's rmse: 7464.66#011val's rmse: 8478.35\u001b[0m\n",
      "\u001b[34m[550]#011train's rmse: 7463.27#011val's rmse: 8477.93\u001b[0m\n",
      "\u001b[34m[551]#011train's rmse: 7462#011val's rmse: 8477.88\u001b[0m\n",
      "\u001b[34m[552]#011train's rmse: 7460.42#011val's rmse: 8477.6\u001b[0m\n",
      "\u001b[34m[553]#011train's rmse: 7458.63#011val's rmse: 8477.58\u001b[0m\n",
      "\u001b[34m[554]#011train's rmse: 7456.96#011val's rmse: 8477.64\u001b[0m\n",
      "\u001b[34m[555]#011train's rmse: 7454.99#011val's rmse: 8477.45\u001b[0m\n",
      "\u001b[34m[556]#011train's rmse: 7453.6#011val's rmse: 8476.73\u001b[0m\n",
      "\u001b[34m[557]#011train's rmse: 7452.04#011val's rmse: 8476.76\u001b[0m\n",
      "\u001b[34m[558]#011train's rmse: 7450.43#011val's rmse: 8476.48\u001b[0m\n",
      "\u001b[34m[559]#011train's rmse: 7448.85#011val's rmse: 8476.36\u001b[0m\n",
      "\u001b[34m[560]#011train's rmse: 7447.47#011val's rmse: 8475.92\u001b[0m\n",
      "\u001b[34m[561]#011train's rmse: 7445.91#011val's rmse: 8475.68\u001b[0m\n",
      "\u001b[34m[562]#011train's rmse: 7443.75#011val's rmse: 8475.84\u001b[0m\n",
      "\u001b[34m[563]#011train's rmse: 7442.17#011val's rmse: 8475.55\u001b[0m\n",
      "\u001b[34m[564]#011train's rmse: 7440.52#011val's rmse: 8475.46\u001b[0m\n",
      "\u001b[34m[565]#011train's rmse: 7438.83#011val's rmse: 8475.61\u001b[0m\n",
      "\u001b[34m[566]#011train's rmse: 7437.58#011val's rmse: 8475.18\u001b[0m\n",
      "\u001b[34m[567]#011train's rmse: 7436.04#011val's rmse: 8474.74\u001b[0m\n",
      "\u001b[34m[568]#011train's rmse: 7434.27#011val's rmse: 8475.02\u001b[0m\n",
      "\u001b[34m[569]#011train's rmse: 7432.9#011val's rmse: 8475.31\u001b[0m\n",
      "\u001b[34m[570]#011train's rmse: 7431.56#011val's rmse: 8475.01\u001b[0m\n",
      "\u001b[34m[571]#011train's rmse: 7429.95#011val's rmse: 8474.63\u001b[0m\n",
      "\u001b[34m[572]#011train's rmse: 7428.4#011val's rmse: 8474.24\u001b[0m\n",
      "\u001b[34m[573]#011train's rmse: 7427.26#011val's rmse: 8473.96\u001b[0m\n",
      "\u001b[34m[574]#011train's rmse: 7426#011val's rmse: 8473.62\u001b[0m\n",
      "\u001b[34m[575]#011train's rmse: 7424.29#011val's rmse: 8473.9\u001b[0m\n",
      "\u001b[34m[576]#011train's rmse: 7422.63#011val's rmse: 8473.86\u001b[0m\n",
      "\u001b[34m[577]#011train's rmse: 7420.91#011val's rmse: 8474.02\u001b[0m\n",
      "\u001b[34m[578]#011train's rmse: 7419.41#011val's rmse: 8474.12\u001b[0m\n",
      "\u001b[34m[579]#011train's rmse: 7418.45#011val's rmse: 8473.9\u001b[0m\n",
      "\u001b[34m[580]#011train's rmse: 7416.91#011val's rmse: 8474.05\u001b[0m\n",
      "\u001b[34m[581]#011train's rmse: 7415.35#011val's rmse: 8473.79\u001b[0m\n",
      "\u001b[34m[582]#011train's rmse: 7413.9#011val's rmse: 8473.7\u001b[0m\n",
      "\u001b[34m[583]#011train's rmse: 7412.45#011val's rmse: 8473.36\u001b[0m\n",
      "\u001b[34m[584]#011train's rmse: 7410.76#011val's rmse: 8472.96\u001b[0m\n",
      "\u001b[34m[585]#011train's rmse: 7409.16#011val's rmse: 8472.8\u001b[0m\n",
      "\u001b[34m[586]#011train's rmse: 7407.5#011val's rmse: 8472.49\u001b[0m\n",
      "\u001b[34m[587]#011train's rmse: 7405.91#011val's rmse: 8472.17\u001b[0m\n",
      "\u001b[34m[588]#011train's rmse: 7404.14#011val's rmse: 8471.74\u001b[0m\n",
      "\u001b[34m[589]#011train's rmse: 7402.51#011val's rmse: 8471.23\u001b[0m\n",
      "\u001b[34m[590]#011train's rmse: 7401.16#011val's rmse: 8470.69\u001b[0m\n",
      "\u001b[34m[591]#011train's rmse: 7399.66#011val's rmse: 8470.41\u001b[0m\n",
      "\u001b[34m[592]#011train's rmse: 7397.98#011val's rmse: 8470.44\u001b[0m\n",
      "\u001b[34m[593]#011train's rmse: 7396.55#011val's rmse: 8470.44\u001b[0m\n",
      "\u001b[34m[594]#011train's rmse: 7394.96#011val's rmse: 8470.38\u001b[0m\n",
      "\u001b[34m[595]#011train's rmse: 7393.55#011val's rmse: 8470.23\u001b[0m\n",
      "\u001b[34m[596]#011train's rmse: 7392.16#011val's rmse: 8470.31\u001b[0m\n",
      "\u001b[34m[597]#011train's rmse: 7390.58#011val's rmse: 8469.95\u001b[0m\n",
      "\u001b[34m[598]#011train's rmse: 7389.08#011val's rmse: 8469.94\u001b[0m\n",
      "\u001b[34m[599]#011train's rmse: 7387.93#011val's rmse: 8469.59\u001b[0m\n",
      "\u001b[34m[600]#011train's rmse: 7386.62#011val's rmse: 8469.43\u001b[0m\n",
      "\u001b[34m[601]#011train's rmse: 7385.38#011val's rmse: 8469.2\u001b[0m\n",
      "\u001b[34m[602]#011train's rmse: 7383.73#011val's rmse: 8469.14\u001b[0m\n",
      "\u001b[34m[603]#011train's rmse: 7382.58#011val's rmse: 8468.95\u001b[0m\n",
      "\u001b[34m[604]#011train's rmse: 7381.28#011val's rmse: 8468.89\u001b[0m\n",
      "\u001b[34m[605]#011train's rmse: 7379.99#011val's rmse: 8468.5\u001b[0m\n",
      "\u001b[34m[606]#011train's rmse: 7378.79#011val's rmse: 8468.3\u001b[0m\n",
      "\u001b[34m[607]#011train's rmse: 7377.33#011val's rmse: 8468.22\u001b[0m\n",
      "\u001b[34m[608]#011train's rmse: 7375.81#011val's rmse: 8468.55\u001b[0m\n",
      "\u001b[34m[609]#011train's rmse: 7374.74#011val's rmse: 8468.14\u001b[0m\n",
      "\u001b[34m[610]#011train's rmse: 7373.08#011val's rmse: 8467.28\u001b[0m\n",
      "\u001b[34m[611]#011train's rmse: 7371.72#011val's rmse: 8467.2\u001b[0m\n",
      "\u001b[34m[612]#011train's rmse: 7370.34#011val's rmse: 8466.66\u001b[0m\n",
      "\u001b[34m[613]#011train's rmse: 7368.7#011val's rmse: 8466.12\u001b[0m\n",
      "\u001b[34m[614]#011train's rmse: 7367.53#011val's rmse: 8465.59\u001b[0m\n",
      "\u001b[34m[615]#011train's rmse: 7366.1#011val's rmse: 8465.56\u001b[0m\n",
      "\u001b[34m[616]#011train's rmse: 7364.68#011val's rmse: 8465.38\u001b[0m\n",
      "\u001b[34m[617]#011train's rmse: 7363.29#011val's rmse: 8465.03\u001b[0m\n",
      "\u001b[34m[618]#011train's rmse: 7361.96#011val's rmse: 8464.96\u001b[0m\n",
      "\u001b[34m[619]#011train's rmse: 7360.34#011val's rmse: 8464.51\u001b[0m\n",
      "\u001b[34m[620]#011train's rmse: 7359.01#011val's rmse: 8464.22\u001b[0m\n",
      "\u001b[34m[621]#011train's rmse: 7357.68#011val's rmse: 8464.26\u001b[0m\n",
      "\u001b[34m[622]#011train's rmse: 7356.24#011val's rmse: 8464.42\u001b[0m\n",
      "\u001b[34m[623]#011train's rmse: 7354.79#011val's rmse: 8464.52\u001b[0m\n",
      "\u001b[34m[624]#011train's rmse: 7353.62#011val's rmse: 8464.2\u001b[0m\n",
      "\u001b[34m[625]#011train's rmse: 7352.03#011val's rmse: 8463.83\u001b[0m\n",
      "\u001b[34m[626]#011train's rmse: 7350.81#011val's rmse: 8463.51\u001b[0m\n",
      "\u001b[34m[627]#011train's rmse: 7349.27#011val's rmse: 8463.35\u001b[0m\n",
      "\u001b[34m[628]#011train's rmse: 7347.94#011val's rmse: 8462.93\u001b[0m\n",
      "\u001b[34m[629]#011train's rmse: 7346.65#011val's rmse: 8462.59\u001b[0m\n",
      "\u001b[34m[630]#011train's rmse: 7345.25#011val's rmse: 8462.51\u001b[0m\n",
      "\u001b[34m[631]#011train's rmse: 7344#011val's rmse: 8462.21\u001b[0m\n",
      "\u001b[34m[632]#011train's rmse: 7342.54#011val's rmse: 8462.26\u001b[0m\n",
      "\u001b[34m[633]#011train's rmse: 7341.43#011val's rmse: 8462.11\u001b[0m\n",
      "\u001b[34m[634]#011train's rmse: 7340.04#011val's rmse: 8462.29\u001b[0m\n",
      "\u001b[34m[635]#011train's rmse: 7338.75#011val's rmse: 8461.88\u001b[0m\n",
      "\u001b[34m[636]#011train's rmse: 7337.28#011val's rmse: 8461.45\u001b[0m\n",
      "\u001b[34m[637]#011train's rmse: 7335.83#011val's rmse: 8461.34\u001b[0m\n",
      "\u001b[34m[638]#011train's rmse: 7334.7#011val's rmse: 8461.6\u001b[0m\n",
      "\u001b[34m[639]#011train's rmse: 7333.44#011val's rmse: 8461.52\u001b[0m\n",
      "\u001b[34m[640]#011train's rmse: 7332.08#011val's rmse: 8461.45\u001b[0m\n",
      "\u001b[34m[641]#011train's rmse: 7330.7#011val's rmse: 8461.97\u001b[0m\n",
      "\u001b[34m[642]#011train's rmse: 7329.34#011val's rmse: 8462.14\u001b[0m\n",
      "\u001b[34m[643]#011train's rmse: 7328.13#011val's rmse: 8462.25\u001b[0m\n",
      "\u001b[34m[644]#011train's rmse: 7326.98#011val's rmse: 8462.22\u001b[0m\n",
      "\u001b[34m[645]#011train's rmse: 7326.01#011val's rmse: 8462.21\u001b[0m\n",
      "\u001b[34m[646]#011train's rmse: 7324.22#011val's rmse: 8462.14\u001b[0m\n",
      "\u001b[34m[647]#011train's rmse: 7323.09#011val's rmse: 8461.91\u001b[0m\n",
      "\u001b[34m[648]#011train's rmse: 7321.78#011val's rmse: 8461.56\u001b[0m\n",
      "\u001b[34m[649]#011train's rmse: 7319.97#011val's rmse: 8461.77\u001b[0m\n",
      "\u001b[34m[650]#011train's rmse: 7318.76#011val's rmse: 8461.71\u001b[0m\n",
      "\u001b[34m[651]#011train's rmse: 7317.51#011val's rmse: 8461.67\u001b[0m\n",
      "\u001b[34m[652]#011train's rmse: 7315.92#011val's rmse: 8461.64\u001b[0m\n",
      "\u001b[34m[653]#011train's rmse: 7314.36#011val's rmse: 8462.01\u001b[0m\n",
      "\u001b[34m[654]#011train's rmse: 7312.67#011val's rmse: 8461.36\u001b[0m\n",
      "\u001b[34m[655]#011train's rmse: 7311.13#011val's rmse: 8461.36\u001b[0m\n",
      "\u001b[34m[656]#011train's rmse: 7310.11#011val's rmse: 8461.49\u001b[0m\n",
      "\u001b[34m[657]#011train's rmse: 7308.67#011val's rmse: 8461.12\u001b[0m\n",
      "\u001b[34m[658]#011train's rmse: 7307.3#011val's rmse: 8461.11\u001b[0m\n",
      "\u001b[34m[659]#011train's rmse: 7306.15#011val's rmse: 8461.24\u001b[0m\n",
      "\u001b[34m[660]#011train's rmse: 7304.98#011val's rmse: 8461.13\u001b[0m\n",
      "\u001b[34m[661]#011train's rmse: 7303.48#011val's rmse: 8461.08\u001b[0m\n",
      "\u001b[34m[662]#011train's rmse: 7302.08#011val's rmse: 8461.44\u001b[0m\n",
      "\u001b[34m[663]#011train's rmse: 7300.89#011val's rmse: 8461.64\u001b[0m\n",
      "\u001b[34m[664]#011train's rmse: 7299.5#011val's rmse: 8461.93\u001b[0m\n",
      "\u001b[34m[665]#011train's rmse: 7298.15#011val's rmse: 8461.75\u001b[0m\n",
      "\u001b[34m[666]#011train's rmse: 7297.06#011val's rmse: 8461.6\u001b[0m\n",
      "\u001b[34m[667]#011train's rmse: 7295.87#011val's rmse: 8461.19\u001b[0m\n",
      "\u001b[34m[668]#011train's rmse: 7294.22#011val's rmse: 8460.99\u001b[0m\n",
      "\u001b[34m[669]#011train's rmse: 7292.9#011val's rmse: 8460.45\u001b[0m\n",
      "\u001b[34m[670]#011train's rmse: 7291.66#011val's rmse: 8459.98\u001b[0m\n",
      "\u001b[34m[671]#011train's rmse: 7290.73#011val's rmse: 8459.99\u001b[0m\n",
      "\u001b[34m[672]#011train's rmse: 7289.78#011val's rmse: 8460\u001b[0m\n",
      "\u001b[34m[673]#011train's rmse: 7288.66#011val's rmse: 8459.99\u001b[0m\n",
      "\u001b[34m[674]#011train's rmse: 7287.74#011val's rmse: 8459.57\u001b[0m\n",
      "\u001b[34m[675]#011train's rmse: 7286.82#011val's rmse: 8459.37\u001b[0m\n",
      "\u001b[34m[676]#011train's rmse: 7285.9#011val's rmse: 8459.55\u001b[0m\n",
      "\u001b[34m[677]#011train's rmse: 7285.08#011val's rmse: 8459.76\u001b[0m\n",
      "\u001b[34m[678]#011train's rmse: 7284.21#011val's rmse: 8460.11\u001b[0m\n",
      "\u001b[34m[679]#011train's rmse: 7283.08#011val's rmse: 8460.38\u001b[0m\n",
      "\u001b[34m[680]#011train's rmse: 7281.79#011val's rmse: 8460.37\u001b[0m\n",
      "\u001b[34m[681]#011train's rmse: 7280.37#011val's rmse: 8460.29\u001b[0m\n",
      "\u001b[34m[682]#011train's rmse: 7279.45#011val's rmse: 8460.26\u001b[0m\n",
      "\u001b[34m[683]#011train's rmse: 7277.93#011val's rmse: 8460.52\u001b[0m\n",
      "\u001b[34m[684]#011train's rmse: 7276.39#011val's rmse: 8460.2\u001b[0m\n",
      "\u001b[34m[685]#011train's rmse: 7275.58#011val's rmse: 8459.92\u001b[0m\n",
      "\u001b[34m[686]#011train's rmse: 7274.19#011val's rmse: 8459.62\u001b[0m\n",
      "\u001b[34m[687]#011train's rmse: 7272.7#011val's rmse: 8459.65\u001b[0m\n",
      "\u001b[34m[688]#011train's rmse: 7271.26#011val's rmse: 8459.68\u001b[0m\n",
      "\u001b[34m[689]#011train's rmse: 7270.31#011val's rmse: 8459.5\u001b[0m\n",
      "\u001b[34m[690]#011train's rmse: 7268.77#011val's rmse: 8459.16\u001b[0m\n",
      "\u001b[34m[691]#011train's rmse: 7267.77#011val's rmse: 8459.03\u001b[0m\n",
      "\u001b[34m[692]#011train's rmse: 7266.55#011val's rmse: 8459.04\u001b[0m\n",
      "\u001b[34m[693]#011train's rmse: 7265.27#011val's rmse: 8459.28\u001b[0m\n",
      "\u001b[34m[694]#011train's rmse: 7264.04#011val's rmse: 8459.03\u001b[0m\n",
      "\u001b[34m[695]#011train's rmse: 7262.76#011val's rmse: 8458.78\u001b[0m\n",
      "\u001b[34m[696]#011train's rmse: 7261.77#011val's rmse: 8458.83\u001b[0m\n",
      "\u001b[34m[697]#011train's rmse: 7260.68#011val's rmse: 8458.53\u001b[0m\n",
      "\u001b[34m[698]#011train's rmse: 7259.64#011val's rmse: 8458.71\u001b[0m\n",
      "\u001b[34m[699]#011train's rmse: 7258.51#011val's rmse: 8458.7\u001b[0m\n",
      "\u001b[34m[700]#011train's rmse: 7257.26#011val's rmse: 8458.38\u001b[0m\n",
      "\u001b[34m[701]#011train's rmse: 7255.7#011val's rmse: 8457.78\u001b[0m\n",
      "\u001b[34m[702]#011train's rmse: 7254.58#011val's rmse: 8457.72\u001b[0m\n",
      "\u001b[34m[703]#011train's rmse: 7253.4#011val's rmse: 8457.77\u001b[0m\n",
      "\u001b[34m[704]#011train's rmse: 7252.33#011val's rmse: 8457.53\u001b[0m\n",
      "\u001b[34m[705]#011train's rmse: 7251.23#011val's rmse: 8457.34\u001b[0m\n",
      "\u001b[34m[706]#011train's rmse: 7250.11#011val's rmse: 8457.35\u001b[0m\n",
      "\u001b[34m[707]#011train's rmse: 7248.86#011val's rmse: 8456.95\u001b[0m\n",
      "\u001b[34m[708]#011train's rmse: 7247.86#011val's rmse: 8456.49\u001b[0m\n",
      "\u001b[34m[709]#011train's rmse: 7246.45#011val's rmse: 8456.58\u001b[0m\n",
      "\u001b[34m[710]#011train's rmse: 7245.33#011val's rmse: 8456.83\u001b[0m\n",
      "\u001b[34m[711]#011train's rmse: 7244.2#011val's rmse: 8457.36\u001b[0m\n",
      "\u001b[34m[712]#011train's rmse: 7243.11#011val's rmse: 8457.58\u001b[0m\n",
      "\u001b[34m[713]#011train's rmse: 7241.76#011val's rmse: 8457.61\u001b[0m\n",
      "\u001b[34m[714]#011train's rmse: 7240.76#011val's rmse: 8457.82\u001b[0m\n",
      "\u001b[34m[715]#011train's rmse: 7239.62#011val's rmse: 8458.05\u001b[0m\n",
      "\u001b[34m[716]#011train's rmse: 7238.6#011val's rmse: 8457.93\u001b[0m\n",
      "\u001b[34m[717]#011train's rmse: 7237.25#011val's rmse: 8457.64\u001b[0m\n",
      "\u001b[34m[718]#011train's rmse: 7236.11#011val's rmse: 8457.98\u001b[0m\n",
      "\u001b[34m[719]#011train's rmse: 7235.03#011val's rmse: 8457.7\u001b[0m\n",
      "\u001b[34m[720]#011train's rmse: 7234.15#011val's rmse: 8457.54\u001b[0m\n",
      "\u001b[34m[721]#011train's rmse: 7233.06#011val's rmse: 8456.95\u001b[0m\n",
      "\u001b[34m[722]#011train's rmse: 7231.88#011val's rmse: 8457.12\u001b[0m\n",
      "\u001b[34m[723]#011train's rmse: 7230.82#011val's rmse: 8456.99\u001b[0m\n",
      "\u001b[34m[724]#011train's rmse: 7229.9#011val's rmse: 8456.94\u001b[0m\n",
      "\u001b[34m[725]#011train's rmse: 7228.92#011val's rmse: 8456.88\u001b[0m\n",
      "\u001b[34m[726]#011train's rmse: 7227.37#011val's rmse: 8456.96\u001b[0m\n",
      "\u001b[34m[727]#011train's rmse: 7226.11#011val's rmse: 8457.2\u001b[0m\n",
      "\u001b[34m[728]#011train's rmse: 7225.08#011val's rmse: 8457.14\u001b[0m\n",
      "\u001b[34m[729]#011train's rmse: 7223.86#011val's rmse: 8457.06\u001b[0m\n",
      "\u001b[34m[730]#011train's rmse: 7222.99#011val's rmse: 8457.04\u001b[0m\n",
      "\u001b[34m[731]#011train's rmse: 7221.67#011val's rmse: 8456.54\u001b[0m\n",
      "\u001b[34m[732]#011train's rmse: 7220.37#011val's rmse: 8455.93\u001b[0m\n",
      "\u001b[34m[733]#011train's rmse: 7219.37#011val's rmse: 8455.84\u001b[0m\n",
      "\u001b[34m[734]#011train's rmse: 7218.29#011val's rmse: 8455.69\u001b[0m\n",
      "\u001b[34m[735]#011train's rmse: 7217.13#011val's rmse: 8455.68\u001b[0m\n",
      "\u001b[34m[736]#011train's rmse: 7216.09#011val's rmse: 8455.58\u001b[0m\n",
      "\u001b[34m[737]#011train's rmse: 7214.96#011val's rmse: 8455.2\u001b[0m\n",
      "\u001b[34m[738]#011train's rmse: 7213.83#011val's rmse: 8455.41\u001b[0m\n",
      "\u001b[34m[739]#011train's rmse: 7212.81#011val's rmse: 8455.59\u001b[0m\n",
      "\u001b[34m[740]#011train's rmse: 7211.7#011val's rmse: 8455.68\u001b[0m\n",
      "\u001b[34m[741]#011train's rmse: 7210.46#011val's rmse: 8455.61\u001b[0m\n",
      "\u001b[34m[742]#011train's rmse: 7208.98#011val's rmse: 8455.01\u001b[0m\n",
      "\u001b[34m[743]#011train's rmse: 7207.87#011val's rmse: 8454.91\u001b[0m\n",
      "\u001b[34m[744]#011train's rmse: 7206.74#011val's rmse: 8455\u001b[0m\n",
      "\u001b[34m[745]#011train's rmse: 7205.31#011val's rmse: 8454.8\u001b[0m\n",
      "\u001b[34m[746]#011train's rmse: 7204.19#011val's rmse: 8454.57\u001b[0m\n",
      "\u001b[34m[747]#011train's rmse: 7202.78#011val's rmse: 8454.56\u001b[0m\n",
      "\u001b[34m[748]#011train's rmse: 7201.65#011val's rmse: 8454.44\u001b[0m\n",
      "\u001b[34m[749]#011train's rmse: 7200.43#011val's rmse: 8454.03\u001b[0m\n",
      "\u001b[34m[750]#011train's rmse: 7199.38#011val's rmse: 8454.26\u001b[0m\n",
      "\u001b[34m[751]#011train's rmse: 7198.14#011val's rmse: 8454.39\u001b[0m\n",
      "\u001b[34m[752]#011train's rmse: 7196.57#011val's rmse: 8454.48\u001b[0m\n",
      "\u001b[34m[753]#011train's rmse: 7195.01#011val's rmse: 8454.2\u001b[0m\n",
      "\u001b[34m[754]#011train's rmse: 7193.84#011val's rmse: 8454.2\u001b[0m\n",
      "\u001b[34m[755]#011train's rmse: 7192.5#011val's rmse: 8454.58\u001b[0m\n",
      "\u001b[34m[756]#011train's rmse: 7191.44#011val's rmse: 8454.7\u001b[0m\n",
      "\u001b[34m[757]#011train's rmse: 7190.16#011val's rmse: 8454.65\u001b[0m\n",
      "\u001b[34m[758]#011train's rmse: 7189.25#011val's rmse: 8454.4\u001b[0m\n",
      "\u001b[34m[759]#011train's rmse: 7187.91#011val's rmse: 8454.24\u001b[0m\n",
      "\u001b[34m[760]#011train's rmse: 7186.7#011val's rmse: 8454.39\u001b[0m\n",
      "\u001b[34m[761]#011train's rmse: 7185.39#011val's rmse: 8454.14\u001b[0m\n",
      "\u001b[34m[762]#011train's rmse: 7184.07#011val's rmse: 8453.75\u001b[0m\n",
      "\u001b[34m[763]#011train's rmse: 7182.8#011val's rmse: 8453.51\u001b[0m\n",
      "\u001b[34m[764]#011train's rmse: 7181.6#011val's rmse: 8453.59\u001b[0m\n",
      "\u001b[34m[765]#011train's rmse: 7180.37#011val's rmse: 8453.36\u001b[0m\n",
      "\u001b[34m[766]#011train's rmse: 7179.43#011val's rmse: 8453.1\u001b[0m\n",
      "\u001b[34m[767]#011train's rmse: 7178.1#011val's rmse: 8452.95\u001b[0m\n",
      "\u001b[34m[768]#011train's rmse: 7177.01#011val's rmse: 8452.6\u001b[0m\n",
      "\u001b[34m[769]#011train's rmse: 7175.95#011val's rmse: 8452.54\u001b[0m\n",
      "\u001b[34m[770]#011train's rmse: 7174.75#011val's rmse: 8452.64\u001b[0m\n",
      "\u001b[34m[771]#011train's rmse: 7173.53#011val's rmse: 8452.81\u001b[0m\n",
      "\u001b[34m[772]#011train's rmse: 7172.46#011val's rmse: 8453.07\u001b[0m\n",
      "\u001b[34m[773]#011train's rmse: 7171.44#011val's rmse: 8453.56\u001b[0m\n",
      "\u001b[34m[774]#011train's rmse: 7170.51#011val's rmse: 8453.45\u001b[0m\n",
      "\u001b[34m[775]#011train's rmse: 7169.52#011val's rmse: 8453.55\u001b[0m\n",
      "\u001b[34m[776]#011train's rmse: 7168.28#011val's rmse: 8453.34\u001b[0m\n",
      "\u001b[34m[777]#011train's rmse: 7166.98#011val's rmse: 8453.26\u001b[0m\n",
      "\u001b[34m[778]#011train's rmse: 7165.81#011val's rmse: 8453.27\u001b[0m\n",
      "\u001b[34m[779]#011train's rmse: 7164.9#011val's rmse: 8453.08\u001b[0m\n",
      "\u001b[34m[780]#011train's rmse: 7163.82#011val's rmse: 8453.16\u001b[0m\n",
      "\u001b[34m[781]#011train's rmse: 7162.78#011val's rmse: 8452.6\u001b[0m\n",
      "\u001b[34m[782]#011train's rmse: 7161.75#011val's rmse: 8452.47\u001b[0m\n",
      "\u001b[34m[783]#011train's rmse: 7160.71#011val's rmse: 8452.52\u001b[0m\n",
      "\u001b[34m[784]#011train's rmse: 7159.58#011val's rmse: 8452.52\u001b[0m\n",
      "\u001b[34m[785]#011train's rmse: 7158.55#011val's rmse: 8452.37\u001b[0m\n",
      "\u001b[34m[786]#011train's rmse: 7157.43#011val's rmse: 8452.25\u001b[0m\n",
      "\u001b[34m[787]#011train's rmse: 7156.16#011val's rmse: 8452.51\u001b[0m\n",
      "\u001b[34m[788]#011train's rmse: 7155.22#011val's rmse: 8452.33\u001b[0m\n",
      "\u001b[34m[789]#011train's rmse: 7154.27#011val's rmse: 8452.33\u001b[0m\n",
      "\u001b[34m[790]#011train's rmse: 7153.12#011val's rmse: 8452.17\u001b[0m\n",
      "\u001b[34m[791]#011train's rmse: 7152.31#011val's rmse: 8451.51\u001b[0m\n",
      "\u001b[34m[792]#011train's rmse: 7151.34#011val's rmse: 8451.34\u001b[0m\n",
      "\u001b[34m[793]#011train's rmse: 7150.19#011val's rmse: 8450.94\u001b[0m\n",
      "\u001b[34m[794]#011train's rmse: 7149.19#011val's rmse: 8450.8\u001b[0m\n",
      "\u001b[34m[795]#011train's rmse: 7148#011val's rmse: 8450.65\u001b[0m\n",
      "\u001b[34m[796]#011train's rmse: 7147.1#011val's rmse: 8450.27\u001b[0m\n",
      "\u001b[34m[797]#011train's rmse: 7146.25#011val's rmse: 8450.04\u001b[0m\n",
      "\u001b[34m[798]#011train's rmse: 7145.34#011val's rmse: 8449.77\u001b[0m\n",
      "\u001b[34m[799]#011train's rmse: 7144.53#011val's rmse: 8449.64\u001b[0m\n",
      "\u001b[34m[800]#011train's rmse: 7143.79#011val's rmse: 8449.4\u001b[0m\n",
      "\u001b[34m[801]#011train's rmse: 7142.77#011val's rmse: 8449.17\u001b[0m\n",
      "\u001b[34m[802]#011train's rmse: 7141.7#011val's rmse: 8448.98\u001b[0m\n",
      "\u001b[34m[803]#011train's rmse: 7140.55#011val's rmse: 8448.54\u001b[0m\n",
      "\u001b[34m[804]#011train's rmse: 7139.01#011val's rmse: 8448.65\u001b[0m\n",
      "\u001b[34m[805]#011train's rmse: 7138.19#011val's rmse: 8448.38\u001b[0m\n",
      "\u001b[34m[806]#011train's rmse: 7136.92#011val's rmse: 8448.56\u001b[0m\n",
      "\u001b[34m[807]#011train's rmse: 7135.57#011val's rmse: 8448.75\u001b[0m\n",
      "\u001b[34m[808]#011train's rmse: 7134.23#011val's rmse: 8448.77\u001b[0m\n",
      "\u001b[34m[809]#011train's rmse: 7133.12#011val's rmse: 8448.86\u001b[0m\n",
      "\u001b[34m[810]#011train's rmse: 7132.02#011val's rmse: 8448.68\u001b[0m\n",
      "\u001b[34m[811]#011train's rmse: 7130.85#011val's rmse: 8448.52\u001b[0m\n",
      "\u001b[34m[812]#011train's rmse: 7129.63#011val's rmse: 8448.59\u001b[0m\n",
      "\u001b[34m[813]#011train's rmse: 7128.49#011val's rmse: 8448.47\u001b[0m\n",
      "\u001b[34m[814]#011train's rmse: 7127.41#011val's rmse: 8448.5\u001b[0m\n",
      "\u001b[34m[815]#011train's rmse: 7126.38#011val's rmse: 8448.39\u001b[0m\n",
      "\u001b[34m[816]#011train's rmse: 7125.06#011val's rmse: 8448.65\u001b[0m\n",
      "\u001b[34m[817]#011train's rmse: 7124.03#011val's rmse: 8448.66\u001b[0m\n",
      "\u001b[34m[818]#011train's rmse: 7122.94#011val's rmse: 8448.38\u001b[0m\n",
      "\u001b[34m[819]#011train's rmse: 7122.07#011val's rmse: 8448.42\u001b[0m\n",
      "\u001b[34m[820]#011train's rmse: 7121.18#011val's rmse: 8448.14\u001b[0m\n",
      "\u001b[34m[821]#011train's rmse: 7119.69#011val's rmse: 8448.51\u001b[0m\n",
      "\u001b[34m[822]#011train's rmse: 7118.44#011val's rmse: 8448.75\u001b[0m\n",
      "\u001b[34m[823]#011train's rmse: 7117.19#011val's rmse: 8448.82\u001b[0m\n",
      "\u001b[34m[824]#011train's rmse: 7116.23#011val's rmse: 8449.09\u001b[0m\n",
      "\u001b[34m[825]#011train's rmse: 7115.36#011val's rmse: 8448.95\u001b[0m\n",
      "\u001b[34m[826]#011train's rmse: 7114.08#011val's rmse: 8448.84\u001b[0m\n",
      "\u001b[34m[827]#011train's rmse: 7113.27#011val's rmse: 8448.94\u001b[0m\n",
      "\u001b[34m[828]#011train's rmse: 7112.18#011val's rmse: 8449.04\u001b[0m\n",
      "\u001b[34m[829]#011train's rmse: 7110.78#011val's rmse: 8448.83\u001b[0m\n",
      "\u001b[34m[830]#011train's rmse: 7109.4#011val's rmse: 8449.19\u001b[0m\n",
      "\u001b[34m[831]#011train's rmse: 7108.35#011val's rmse: 8449.27\u001b[0m\n",
      "\u001b[34m[832]#011train's rmse: 7107.43#011val's rmse: 8449.66\u001b[0m\n",
      "\u001b[34m[833]#011train's rmse: 7106.32#011val's rmse: 8449.76\u001b[0m\n",
      "\u001b[34m[834]#011train's rmse: 7105.24#011val's rmse: 8449.64\u001b[0m\n",
      "\u001b[34m[835]#011train's rmse: 7104.39#011val's rmse: 8449.73\u001b[0m\n",
      "\u001b[34m[836]#011train's rmse: 7103.16#011val's rmse: 8449.7\u001b[0m\n",
      "\u001b[34m[837]#011train's rmse: 7102.18#011val's rmse: 8449.68\u001b[0m\n",
      "\u001b[34m[838]#011train's rmse: 7100.92#011val's rmse: 8449.81\u001b[0m\n",
      "\u001b[34m[839]#011train's rmse: 7099.93#011val's rmse: 8449.16\u001b[0m\n",
      "\u001b[34m[840]#011train's rmse: 7098.8#011val's rmse: 8448.8\u001b[0m\n",
      "\u001b[34m[841]#011train's rmse: 7097.73#011val's rmse: 8448.92\u001b[0m\n",
      "\u001b[34m[842]#011train's rmse: 7096.37#011val's rmse: 8448.91\u001b[0m\n",
      "\u001b[34m[843]#011train's rmse: 7095.27#011val's rmse: 8448.45\u001b[0m\n",
      "\u001b[34m[844]#011train's rmse: 7094.01#011val's rmse: 8448.32\u001b[0m\n",
      "\u001b[34m[845]#011train's rmse: 7092.74#011val's rmse: 8447.99\u001b[0m\n",
      "\u001b[34m[846]#011train's rmse: 7091.84#011val's rmse: 8448\u001b[0m\n",
      "\u001b[34m[847]#011train's rmse: 7090.97#011val's rmse: 8448.14\u001b[0m\n",
      "\u001b[34m[848]#011train's rmse: 7090.04#011val's rmse: 8448.19\u001b[0m\n",
      "\u001b[34m[849]#011train's rmse: 7089.12#011val's rmse: 8448.26\u001b[0m\n",
      "\u001b[34m[850]#011train's rmse: 7088.01#011val's rmse: 8448.21\u001b[0m\n",
      "\u001b[34m[851]#011train's rmse: 7086.52#011val's rmse: 8448.46\u001b[0m\n",
      "\u001b[34m[852]#011train's rmse: 7085.34#011val's rmse: 8448.73\u001b[0m\n",
      "\u001b[34m[853]#011train's rmse: 7083.92#011val's rmse: 8449.01\u001b[0m\n",
      "\u001b[34m[854]#011train's rmse: 7082.69#011val's rmse: 8449.43\u001b[0m\n",
      "\u001b[34m[855]#011train's rmse: 7081.44#011val's rmse: 8449.57\u001b[0m\n",
      "\u001b[34m[856]#011train's rmse: 7080.26#011val's rmse: 8449.35\u001b[0m\n",
      "\u001b[34m[857]#011train's rmse: 7079.29#011val's rmse: 8449.54\u001b[0m\n",
      "\u001b[34m[858]#011train's rmse: 7078.05#011val's rmse: 8449.72\u001b[0m\n",
      "\u001b[34m[859]#011train's rmse: 7076.84#011val's rmse: 8449.62\u001b[0m\n",
      "\u001b[34m[860]#011train's rmse: 7075.75#011val's rmse: 8449.79\u001b[0m\n",
      "\u001b[34m[861]#011train's rmse: 7074.61#011val's rmse: 8449.99\u001b[0m\n",
      "\u001b[34m[862]#011train's rmse: 7073.69#011val's rmse: 8450.01\u001b[0m\n",
      "\u001b[34m[863]#011train's rmse: 7072.66#011val's rmse: 8449.92\u001b[0m\n",
      "\u001b[34m[864]#011train's rmse: 7071.65#011val's rmse: 8450.23\u001b[0m\n",
      "\u001b[34m[865]#011train's rmse: 7070.59#011val's rmse: 8450.51\u001b[0m\n",
      "\u001b[34m[866]#011train's rmse: 7069.49#011val's rmse: 8450.52\u001b[0m\n",
      "\u001b[34m[867]#011train's rmse: 7068.43#011val's rmse: 8450.68\u001b[0m\n",
      "\u001b[34m[868]#011train's rmse: 7067.49#011val's rmse: 8450.64\u001b[0m\n",
      "\u001b[34m[869]#011train's rmse: 7066.71#011val's rmse: 8450.63\u001b[0m\n",
      "\u001b[34m[870]#011train's rmse: 7065.92#011val's rmse: 8450.6\u001b[0m\n",
      "\u001b[34m[871]#011train's rmse: 7064.68#011val's rmse: 8450.63\u001b[0m\n",
      "\u001b[34m[872]#011train's rmse: 7063.53#011val's rmse: 8450.62\u001b[0m\n",
      "\u001b[34m[873]#011train's rmse: 7062.23#011val's rmse: 8450.4\u001b[0m\n",
      "\u001b[34m[874]#011train's rmse: 7061.16#011val's rmse: 8450.64\u001b[0m\n",
      "\u001b[34m[875]#011train's rmse: 7059.83#011val's rmse: 8450.74\u001b[0m\n",
      "\u001b[34mEarly stopping, best iteration is:\u001b[0m\n",
      "\u001b[34m[845]#011train's rmse: 7092.74#011val's rmse: 8447.99\u001b[0m\n",
      "\u001b[34mINFO:root:Saving model...\u001b[0m\n",
      "\u001b[34mINFO:root:Info file not found at '_input_model_extracted/__models_info__.json'.\u001b[0m\n",
      "\u001b[34m2023-04-02 21:33:37,516 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2023-04-02 21:33:55 Uploading - Uploading generated training model\n",
      "2023-04-02 21:33:55 Completed - Training job completed\n",
      "Training seconds: 87\n",
      "Billable seconds: 87\n"
     ]
    }
   ],
   "source": [
    "tabular_estimator.fit(\n",
    "        {\"training\": training_dataset_s3_path}, logs=True, job_name=training_job_name\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b9480333-2b39-4d7e-93f6-08707e3d5bba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "inference_instance_type = \"ml.m5.large\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a103bfc6-630a-4cd6-a520-3bb9e6b1b8aa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Retrieve the inference docker container uri\n",
    "deploy_image_uri = image_uris.retrieve(\n",
    "    region=None,\n",
    "    framework=None,\n",
    "    image_scope=\"inference\",\n",
    "    model_id=train_model_id,\n",
    "    model_version=train_model_version,\n",
    "    instance_type=inference_instance_type,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8e7b97ab-b491-41f9-8f45-f991e78c9865",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Retrieve the inference script uri\n",
    "deploy_source_uri = script_uris.retrieve(\n",
    "    model_id=train_model_id, model_version=train_model_version, script_scope=\"inference\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "767cbcf7-67c6-48b6-89af-9682f6d65b77",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "endpoint_name = name_from_base(f\"capstone-example-{train_model_id}-\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3f06d016-6851-4766-baf6-84289cb89c37",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating model with name: sagemaker-jumpstart-2023-04-02-21-34-58-633\n",
      "INFO:sagemaker:Creating endpoint-config with name capstone-example-lightgbm-classificatio-2023-04-02-21-34-58-569\n",
      "INFO:sagemaker:Creating endpoint with name capstone-example-lightgbm-classificatio-2023-04-02-21-34-58-569\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----!"
     ]
    }
   ],
   "source": [
    "predictor = (tabular_estimator).deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=inference_instance_type,\n",
    "    entry_point=\"inference.py\",\n",
    "    image_uri=deploy_image_uri,\n",
    "    source_dir=deploy_source_uri,\n",
    "    endpoint_name=endpoint_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2848c1dc-caad-4c6c-8704-02a20eea54d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
